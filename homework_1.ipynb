{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgNFOSlFEzm"
      },
      "source": [
        "# МОиВС \"Генеративные модели\", 5-й модуль\n",
        "\n",
        "# Homework 1\n",
        "\n",
        "В этой домашней работе вам предстоит добавить к BERT'у декодерную часть и решить задачу генерации суммаризаций для текстов новостей на русском языке.\n",
        "\n",
        "Дополнительно к этому на отличную оценку потребуется реализовать подсчет метрик качества и менее жадную стратегию выбора следующего токена для генерации.\n",
        "\n",
        "*Мы сразу вас предостерегаем попасть в петлю бесконечного дообучения модели. Эта домашка не на пробитие скора. Мы будем проверять, что вы, в целом, сделали все верно и смогли получить какую-то более-менее адекватную (такую, которая заметно лучше той, что была до начала обучения) генерацию. Таким образом, если вы видите, что модель учится, не надо дообучать её сутками. Нескольких часов точно должно хватить.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "По любым вопросам касательно этой домашней работы обращайтесь ко своим ассистентам\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q-oW4ttVEL_9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/sp/.local/lib/python3.10/site-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /home/sp/.local/lib/python3.10/site-packages (3.0.1)\n",
            "Requirement already satisfied: evaluate in /home/sp/.local/lib/python3.10/site-packages (0.4.3)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 KB 788.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in /home/sp/.local/lib/python3.10/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: filelock in /home/sp/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: multiprocess in /home/sp/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /home/sp/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in /home/sp/.local/lib/python3.10/site-packages (from datasets) (3.10.7)\n",
            "Requirement already satisfied: pandas in /home/sp/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /home/sp/.local/lib/python3.10/site-packages (from bert_score) (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /home/sp/.local/lib/python3.10/site-packages (from bert_score) (3.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sp/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
            "Requirement already satisfied: jinja2 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
            "Requirement already satisfied: networkx in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.20.5)\n",
            "Requirement already satisfied: sympy in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sp/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.40)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (3.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (4.51.0)\n",
            "Requirement already satisfied: pillow>=8 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (10.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/sp/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/sp/.local/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/sp/.local/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers datasets evaluate bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ygnbZcjlgJR9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYW38mH0gKX0"
      },
      "source": [
        "## Подготовка данных (0.5 балла)\n",
        "\n",
        "Мы воспользуемся датасетом с 🤗 Ильи Гусева \"gazeta\". Он представляет собой пары (полный текст новости -- его саммари). Пары были взяты с одноименного сайта в домене .ru\n",
        "\n",
        "Более подробно про датасет можно прочитать [здесь](https://huggingface.co/datasets/IlyaGusev/gazeta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mDV4tJzzB5Hi"
      },
      "outputs": [],
      "source": [
        "# Загрузим данные с попощью библиотеки библиотеки datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\", split='train[:5%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjri9a4h6K6"
      },
      "source": [
        "Вы должны помнить, что тексты перед подачей в модель необходимо **токенизировать**.\n",
        "\n",
        "Добавьте паддинг до `max_length=512` для обучающих данных, а также до `max_length=128` для меток.\n",
        "\n",
        "Используйте обрезку текстов, длина которых в токенах превышает `max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подготовим данные для модели Bert\n",
        "\n",
        "model_name = 'deepvk/bert-base-uncased' # Указание модели BERT\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# special_tokens = {'eos_token': '[EOS]'}\n",
        "# tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "def preprocess(examples, use_padding=True):\n",
        "    model_inputs = tokenizer(examples['text'], padding= 'max_length' if use_padding else '', truncation=True, max_length=512)\n",
        "    summary = tokenizer(examples['summary'], padding= 'max_length' if use_padding else '', truncation=True, max_length=128)\n",
        "    model_inputs['labels'] = summary['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a8e5203a02b845a29f57ca545c50c5d0",
            "09b11d6c6eae4979a1c8cd42e32730ae",
            "911fd70fda12476ab2b788433d6ff83f",
            "bcfc58a3f90745449c3f559aa5ab999a",
            "40c29f2dde174a3c8442d9b86a4e3fe9",
            "ab3d20b0e457431dbe24120bb4becede",
            "68b44cb7bddd4a2f950db1a9c00ce066",
            "2665d63c206a45d88fada74bc984771e",
            "c0ddd3783ec047569c2024e461d5ad0f",
            "06437165ba564bff9e29aa53f4c0df5b",
            "665428d410664f94babcd067b879ce2e"
          ]
        },
        "id": "VQxpZ5ivhjlh",
        "outputId": "b3876676-3dc7-4d1d-894e-f0630172afa4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "tokenized_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXQ8gq1UijNj"
      },
      "source": [
        "Размер батча советуем подбирать таким образом, чтоб утилизировать максимум доступной VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xmMCjFAqSDWR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataloader = DataLoader(splitted_dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(splitted_dataset['test'], batch_size=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f67f61a6a40>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 43, 23, 54 ->\n",
        "# 1,  0,  0,  0 -> 43, 0, 0\n",
        "# 1, 43,  0,  0 -> 43, 23, 0\n",
        "# 1, 43, 23,  0 \n",
        "# 1, 43, 23, 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 0],\n",
              "         [1, 1]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([[[1,0,0], [1,1,0]]])\n",
        "# b = torch.cat([torch.full((a.size()[0], a.size()[1], 1), 100), a[:,:-1]], )\n",
        "\n",
        "# torch.full((3, 1, 1,), 100), a\n",
        "# a\n",
        "# b.T, b.transpose(0,1)\n",
        "a[:,:,:-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0J1iEfFHxRz"
      },
      "source": [
        "## Реализация Decoder-cети (3 балла)\n",
        "\n",
        "В данном разделе вам необходимо **реализовать собственный декодер для генерации текста**.\n",
        "\n",
        "Можете вдохновляться кодом с семинара 1 по GPT. В инициализации весов стоит (но необязательно) проявить смекалку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = BertModel.from_pretrained('deepvk/bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.full([a.size()[0], 1], tokenizer.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y5qSblF1EMEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import math\n",
        "\n",
        "# Устанавливаем устройство (GPU, если доступен, иначе CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "nn.Transformer\n",
        "# Класс модели для суммаризации на основе BERT с кастомным декодером\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, device=device)  # Переносим сразу на устройство\n",
        "        position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2, device=device).float() * (-math.log(10000.0) / hidden_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class BertSummarizer(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_decoder_layers=3, num_heads=8, dropout=0.1):\n",
        "        super(BertSummarizer, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(device)  # Переносим модель BERT на устройство\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tokenizer = tokenizer\n",
        "        # Эмбеддинги для токенов на входе в декодер\n",
        "        self.embedding = nn.Embedding(self.bert.config.vocab_size, hidden_size).to(device)  # Переносим на устройство\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size)  # Позиционное кодирование также на устройстве\n",
        "        # Attention головы\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout, batch_first=True).to(device),\n",
        "            num_layers=num_decoder_layers,\n",
        "        ).to(device)  # Переносим декодер на устройство\n",
        "        self.fc_out = nn.Linear(hidden_size, self.bert.config.vocab_size).to(device)  # Линейный слой на устройство\n",
        "        self.softmax = nn.Softmax(dim=2).to(device)\n",
        "\n",
        "    # Функция для создания маски для предотвращения заглядывания вперед в декодере\n",
        "    def generate_square_subsequent_mask(self, T):\n",
        "        return torch.triu(\n",
        "            torch.full((T, T), float('-inf'), device=device, dtype=torch.float64),  # Маска на устройстве\n",
        "            diagonal=1,\n",
        "        )\n",
        "\n",
        "    # def shift_decoder_input(self, input_ids):\n",
        "    #     pad_column = torch.full([input_ids.size()[0], 1], self.tokenizer.pad_token_id, device=device)  # Перенос на устройство\n",
        "    #     return torch.cat([input_ids[:, :-1], pad_column,], dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
        "        # Переносим данные на устройство\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state  # Выходы BERT для использования в декодере\n",
        "        \n",
        "        return self.decoder_forward(decoder_input_ids, memory)\n",
        "\n",
        "    def decoder_forward(self, input_ids, memory):\n",
        "        # shifted_ids = self.shift_decoder_input(input_ids)\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "        decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)  # Маска на устройстве\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "        # print('output_size', output.size())\n",
        "        output = self.fc_out(output)  # Переносим финальный результат на устройство\n",
        "        # print('output_size', output.size())\n",
        "        return output\n",
        "\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_len=50):\n",
        "        # Перенос данных на устройство\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state\n",
        "        memory.to(device)\n",
        "        # print('input_ids.size()', input_ids.size())\n",
        "        # print('encoder_outputs.size()', memory.size())\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Начинаем с токена [CLS] или [BOS] (начало последовательности)\n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long).to(device)\n",
        "        \n",
        "        # memory = memory.transpose(0, 1)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedded = self.embedding(decoder_input_ids)\n",
        "            embedded = self.positional_encoding(embedded)\n",
        "\n",
        "            decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)\n",
        "            # print('decoder_attention_mask.size()', decoder_attention_mask.size())\n",
        "            \n",
        "            decoder_output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "            # print('decoder_output.size()', decoder_output.size())\n",
        "\n",
        "            output = self.fc_out(decoder_output)\n",
        "            # print('output', output.size())\n",
        "\n",
        "            probs = self.softmax(output)\n",
        "            # print('probs', probs.size())\n",
        "            ids = torch.argmax(probs, dim=2)\n",
        "            # print('ids', ids.size())\n",
        "            # print('decoder_input_ids', decoder_input_ids.size())\n",
        "            decoder_input_ids = torch.cat((decoder_input_ids, ids[:, -1:]), dim=1)\n",
        "\n",
        "            if decoder_input_ids[0, -1] == tokenizer.sep_token_id:\n",
        "                break\n",
        "\n",
        "        generated_sequence = tokenizer.decode(decoder_input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return generated_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    1, 20433,   635,  7648, 31235, 15386, 19948, 10230,    16,  4430,\n",
              "          4885,    88,  3750, 14371,  3870,    18, 13955, 13402, 12063,    88,\n",
              "          3772, 13116,   839, 12199,    16,   561,   104, 27683, 25613,   666,\n",
              "         13162,  8349, 32240,   535,   566,  1130,  4348,  6306,  3118,    18,\n",
              "         12239,  7648, 26197, 11960,   296, 14401,   565,  3467,    16,    86,\n",
              "          3323,  3346,  1602,    16,  1127,   625,  1856,    16, 20499,   296,\n",
              "           102, 13402, 15109,   282,    18,     2,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "eval_data_sample['labels'][:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z5VXXCKgecHc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Инициализируем нашу модель и посморим на ее архитектруру\n",
        "\n",
        "# model = BertSummarizer(bert_model_name=model_name, tokenizer=tokenizer)\n",
        "# model = model.to('cuda')\n",
        "# # model\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "\n",
        "# model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)\n",
        "eval_data_sample['input_ids'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H2L-0BmZyu1"
      },
      "source": [
        "## Обучение модели (1 балл)\n",
        "\n",
        "<small> 0.25 балла за простейший рабочий цикл; </small>\n",
        "\n",
        "<small> +0.5 балла за графики для лосса и метрик на трейне и валидации.</small>\n",
        "\n",
        "В данном разделе вам необходимо **реализовать цикл для обучения модели**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(305, 305)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader), len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "us3xiacHBm-U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7f67f4e15360>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # Для отображения прогресса\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "# Выбираем устройство: GPU, если доступно, иначе CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "# Инициализируем модель и переносим её на устройство\n",
        "model = BertSummarizer(bert_model_name=model_name).to(device)\n",
        "\n",
        "\n",
        "def shift_decoder_input(input_ids):\n",
        "    pad_column = torch.full([input_ids.size()[0], 1], tokenizer.pad_token_id, device=device)  # Перенос на устройство\n",
        "    return torch.cat([input_ids[:, 1:], pad_column], dim=1)\n",
        "\n",
        "def train_step(model, input_ids, attention_mask, decoder_input_ids, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    \n",
        "    # Перенос данных на устройство\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    decoder_input_ids = decoder_input_ids.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    optimizer.zero_grad()  # Обнуляем градиенты\n",
        "    outputs = model(input_ids, attention_mask, decoder_input_ids)  # Получаем предсказания\n",
        "    # logits = outputs.reshape(-1, outputs.size(-1)).to(device)\n",
        "    # labels = labels.reshape(-1).to(device)\n",
        "    # Вычисляем лосс, учитывая, что output и decoder_input_ids должны быть в одном устройстве\n",
        "    # print('labels.size', labels.size())\n",
        "\n",
        "    # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "    # labels = labels.reshape(-1)\n",
        "    logits = outputs.reshape(-1, outputs.size(-1))\n",
        "    labels = labels.reshape(-1)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()  # Обратное распространение\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()  # Обновление параметров\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def validate_step(model, input_ids, attention_mask, decoder_input_ids, criterion, device):\n",
        "    model.eval()\n",
        "    \n",
        "    # Перенос данных на устройство\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
        "        outputs = model(input_ids, attention_mask, decoder_input_ids)  # Получаем предсказания\n",
        "        # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "        # Вычисляем лосс для валидации\n",
        "        logits = outputs.reshape(-1, outputs.size(-1))\n",
        "        labels = labels.reshape(-1)\n",
        "        loss = criterion(logits, labels)\n",
        "        # loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Инициализируем функцию потерь и оптимизатор\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)  # Переносим функцию потерь на устройство\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Пример данных (используйте DataLoader для реальных данных)\n",
        "train_data_sample = next(iter(train_dataloader))\n",
        "val_data_sample = next(iter(eval_dataloader))  # Предполагается, что есть отдельный валидирующий даталоадер\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 1\n",
        "plt.ion()  # Включаем интерактивный режим для обновления графика\n",
        "\n",
        "# # Основной цикл обучения\n",
        "# for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "#     running_train_loss = 0.0\n",
        "#     running_val_loss = 0.0\n",
        "    \n",
        "#     # Используем tqdm для прогресса по батчам\n",
        "#     batch_iterator = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    \n",
        "#     # Тренировка\n",
        "#     for batch_idx, sample in enumerate(batch_iterator):\n",
        "#         # Выполняем один шаг обучения и сохраняем лосс\n",
        "#         loss_item = train_step(\n",
        "#             model,\n",
        "#             sample['input_ids'], \n",
        "#             sample['attention_mask'], \n",
        "#             sample['labels'],  # Предполагается, что 'labels' — это целевые токены\n",
        "#             optimizer, \n",
        "#             criterion,\n",
        "#             device\n",
        "#         )\n",
        "#         running_train_loss += loss_item\n",
        "#         train_losses.append(running_train_loss / (batch_idx + 1) )  # Сохраняем текущий лосс\n",
        "\n",
        "#         # Валидация после каждого батча\n",
        "#         model.eval()  # Переводим модель в режим валидации\n",
        "#         val_sample = random.choice(list(train_dataloader))\n",
        "#         val_loss_item = validate_step(\n",
        "#             model,\n",
        "#             val_sample['input_ids'],\n",
        "#             val_sample['attention_mask'],\n",
        "#             val_sample['labels'],  # Предполагается, что 'labels' — это целевые токены для валидации\n",
        "#             criterion,\n",
        "#             device\n",
        "#         )\n",
        "#         running_val_loss += val_loss_item\n",
        "\n",
        "#         if (batch_idx % 30 == 0):\n",
        "#             val_losses.append(running_val_loss / (batch_idx + 1))\n",
        "#             # Обновляем график после каждого батча\n",
        "#             clear_output(wait=True)  # Очищаем старый график\n",
        "#             plt.figure(figsize=(8, 6))\n",
        "#             plt.plot(train_losses, label='Training Loss')\n",
        "#             plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "#             plt.xlabel('Batch')\n",
        "#             plt.ylabel('Loss')\n",
        "#             plt.title(f'Training and Validation Loss (Epoch {epoch+1})')\n",
        "#             plt.grid(True)\n",
        "#             plt.legend()\n",
        "#             plt.show()\n",
        "\n",
        "#     # Средний лосс за эпоху\n",
        "#     epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "#     epoch_val_loss = running_val_loss / len(eval_dataloader)\n",
        "\n",
        "#     # print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "# plt.ioff()  # Отключаем\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'##ъ родины кузне новоросси від пожи обож ##ряс шеф мою грозно постоянным оставить владель просве ##2 постоянным отличным объявляет солны расстав родины кузне надле кузне ##одеи ▶ актер григорь шеф мою расстав родины солны расстав расстав пламенем перестанет удобное ##вами шеф мою грозно солны взаимны солны первома moon prem солны'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = model.to('cuda')\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo01OhsoaacU"
      },
      "source": [
        "## Метрики качества (1 балл)\n",
        "\n",
        "<small>По 0.33 балла за реализацию каждой из предлагаемых метрик</small>\n",
        "\n",
        "**Реализуйте функицию для подсчета метрик качества суммаризации.**\n",
        "\n",
        "Докуметация по некотрым метрикам:\n",
        " 1. [HuggingFace Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        " 2. [HuggingFace Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        " 3. [HuggingFace BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BBNcGXt8aSJ2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b805dd08adb48559f319fb7637142e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  17%|#6        | 241M/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': [1.0, 1.0],\n",
              " 'recall': [1.0, 1.0],\n",
              " 'f1': [1.0, 1.0],\n",
              " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.2)'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [\"hello there\", \"general kenobi\"]\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def compute_metrics():\n",
        "\n",
        "def evaluation():\n",
        "    #<YOUR CODE HERE>\n",
        "    pass\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ5GaAZ1chBu"
      },
      "source": [
        "## Обучение модели (0.5 балла)\n",
        "**Обучите модель, сохраните лучшую версию** (метод `.save_pretrained()` объекта класса AutoModel... или `torch.save()`) **и добавьте пример генерации**. Учтите, что если изменялся токенизатор (а лучше просто по умолчанию), его тоже нужно сохранить. Если планируете продолжить обучение\n",
        "\n",
        "Для сравнения оценки качества генерации по значениям реализованных метрик можете запустить ruT5-small без дообучения. Мы намеренно даем бейзлайн именно в таком виде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHu9RzbQcceV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoModel\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"YOUR MODEL\")\n",
        "summary = #<YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQH_vj6d2Ue"
      },
      "source": [
        "## Реализация менее жадных стратегий выбора следующего токена (4 балла)\n",
        "Всегда ли выбор наиболее вероятного токена на каждом шаге – это лучшая стратегия для генерации текста?\n",
        "\n",
        "<details>\n",
        "    <summary>Спойлер</summary>\n",
        "    <p>Нет</p>\n",
        "</details>\n",
        "\n",
        "**Сравнение стратегий для генерации текста:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br><br/> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br><br/> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words <br/> that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br><br/> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps <br/> the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br><br/> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br><br/> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities <br/> by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br><br/> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br><br/> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br><br/> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Ссылки на докуметацию:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQF4Vc3msKpF"
      },
      "source": [
        "**1. Дополните метод `generate` в модели, чтобы получать топ-k самых вероятных токена и их \"вероятности\"** (1 балл).   \n",
        "\n",
        "**2. Реализуйте стратегию Nucleus Sampling в методе `generate`** (1 балл)\n",
        "\n",
        "**3. Реализуйте стратегию Beam Search** (2 балла)\n",
        "\n",
        "Получилось ли улучшить генерацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfAEfP5kHcc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiksVMOOvO8"
      },
      "source": [
        "## Послевкусие (0 баллов)\n",
        "\n",
        "Если эта домашняя работа показалась вам недостаточно большой, предлагаем провести следующий эксперимент:\n",
        "\n",
        "- от имеющейся модели \"откусить\" только декодерную часть (откусить также можно от ruT5-small);\n",
        "- немного дообучить (что называется, по вкусу);\n",
        "- посмотреть качество генерации по метрикам и \"глазами\";\n",
        "- сравнить полученное с Encoder-Decoder архитектурой;\n",
        "- ответить на вопрос \"Дает ли применение Encoder-Decoder архитектуры значительный буст в качестве генерации, или это некоторый overkill?\" (базово, ответ лежит на поверхности 😸)\n",
        "\n",
        "Ещё более опционально можно:\n",
        "- почитать про возможности генерации Encoder-only архитектурными решениями (BERT, e.g.)\n",
        "- сравнить с генерацией только Decoder'ом и both Encoder-Decoder'ом;\n",
        "- в т.ч. подобрать число обучаемых параметров таким образом, чтоб оно было примерно одинаковым для каждого инстанса моделей (их, инстансов, будет 3 -- только энкодер, только декодер и энкодер-декодер).\n",
        "\n",
        "*Вообще ориентироваться следует на следующее утверждение: \"Только энкодерные архитектуры (BERT, e.g.) хороши для понимания текста (получения эмеддингов), лишь декодерные (GPT, например) -- для генерации, энкодер-декодерные (скажем, T5) -- для обеих задач\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZM1xLliO1QM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06437165ba564bff9e29aa53f4c0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b11d6c6eae4979a1c8cd42e32730ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3d20b0e457431dbe24120bb4becede",
            "placeholder": "​",
            "style": "IPY_MODEL_68b44cb7bddd4a2f950db1a9c00ce066",
            "value": "Map: 100%"
          }
        },
        "2665d63c206a45d88fada74bc984771e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c29f2dde174a3c8442d9b86a4e3fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665428d410664f94babcd067b879ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b44cb7bddd4a2f950db1a9c00ce066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911fd70fda12476ab2b788433d6ff83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2665d63c206a45d88fada74bc984771e",
            "max": 3048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ddd3783ec047569c2024e461d5ad0f",
            "value": 3048
          }
        },
        "a8e5203a02b845a29f57ca545c50c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09b11d6c6eae4979a1c8cd42e32730ae",
              "IPY_MODEL_911fd70fda12476ab2b788433d6ff83f",
              "IPY_MODEL_bcfc58a3f90745449c3f559aa5ab999a"
            ],
            "layout": "IPY_MODEL_40c29f2dde174a3c8442d9b86a4e3fe9"
          }
        },
        "ab3d20b0e457431dbe24120bb4becede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfc58a3f90745449c3f559aa5ab999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06437165ba564bff9e29aa53f4c0df5b",
            "placeholder": "​",
            "style": "IPY_MODEL_665428d410664f94babcd067b879ce2e",
            "value": " 3048/3048 [00:13&lt;00:00, 206.31 examples/s]"
          }
        },
        "c0ddd3783ec047569c2024e461d5ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
