{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgNFOSlFEzm"
      },
      "source": [
        "# МОиВС \"Генеративные модели\", 5-й модуль\n",
        "\n",
        "# Homework 1\n",
        "\n",
        "В этой домашней работе вам предстоит добавить к BERT'у декодерную часть и решить задачу генерации суммаризаций для текстов новостей на русском языке.\n",
        "\n",
        "Дополнительно к этому на отличную оценку потребуется реализовать подсчет метрик качества и менее жадную стратегию выбора следующего токена для генерации.\n",
        "\n",
        "*Мы сразу вас предостерегаем попасть в петлю бесконечного дообучения модели. Эта домашка не на пробитие скора. Мы будем проверять, что вы, в целом, сделали все верно и смогли получить какую-то более-менее адекватную (такую, которая заметно лучше той, что была до начала обучения) генерацию. Таким образом, если вы видите, что модель учится, не надо дообучать её сутками. Нескольких часов точно должно хватить.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "По любым вопросам касательно этой домашней работы обращайтесь ко своим ассистентам\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q-oW4ttVEL_9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (3.0.0)\n",
            "Requirement already satisfied: evaluate in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.4.3)\n",
            "Requirement already satisfied: bert_score in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.3.13)\n",
            "Requirement already satisfied: rouge_score in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from bert_score) (3.8.2)\n",
            "Requirement already satisfied: absl-py in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
            "Requirement already satisfied: nltk in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (6.1.1)\n",
            "Requirement already satisfied: click in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert_score) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers datasets evaluate bert_score rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ygnbZcjlgJR9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYW38mH0gKX0"
      },
      "source": [
        "## Подготовка данных (0.5 балла)\n",
        "\n",
        "Мы воспользуемся датасетом с 🤗 Ильи Гусева \"gazeta\". Он представляет собой пары (полный текст новости -- его саммари). Пары были взяты с одноименного сайта в домене .ru\n",
        "\n",
        "Более подробно про датасет можно прочитать [здесь](https://huggingface.co/datasets/IlyaGusev/gazeta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mDV4tJzzB5Hi"
      },
      "outputs": [],
      "source": [
        "# Загрузим данные с попощью библиотеки библиотеки datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\", split='train[:5%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjri9a4h6K6"
      },
      "source": [
        "Вы должны помнить, что тексты перед подачей в модель необходимо **токенизировать**.\n",
        "\n",
        "Добавьте паддинг до `max_length=512` для обучающих данных, а также до `max_length=128` для меток.\n",
        "\n",
        "Используйте обрезку текстов, длина которых в токенах превышает `max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Подготовим данные для модели Bert\n",
        "\n",
        "model_name = 'deepvk/bert-base-uncased' # Указание модели BERT\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# special_tokens = {'eos_token': '[EOS]'}\n",
        "# tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "def preprocess(examples, use_padding=True):\n",
        "    model_inputs = tokenizer(examples['text'], padding= 'max_length' if use_padding else '', truncation=True, max_length=512)\n",
        "    summary = tokenizer(examples['summary'], padding= 'max_length' if use_padding else '', truncation=True, max_length=128)\n",
        "    model_inputs['labels'] = summary['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a8e5203a02b845a29f57ca545c50c5d0",
            "09b11d6c6eae4979a1c8cd42e32730ae",
            "911fd70fda12476ab2b788433d6ff83f",
            "bcfc58a3f90745449c3f559aa5ab999a",
            "40c29f2dde174a3c8442d9b86a4e3fe9",
            "ab3d20b0e457431dbe24120bb4becede",
            "68b44cb7bddd4a2f950db1a9c00ce066",
            "2665d63c206a45d88fada74bc984771e",
            "c0ddd3783ec047569c2024e461d5ad0f",
            "06437165ba564bff9e29aa53f4c0df5b",
            "665428d410664f94babcd067b879ce2e"
          ]
        },
        "id": "VQxpZ5ivhjlh",
        "outputId": "b3876676-3dc7-4d1d-894e-f0630172afa4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "tokenized_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXQ8gq1UijNj"
      },
      "source": [
        "Размер батча советуем подбирать таким образом, чтоб утилизировать максимум доступной VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xmMCjFAqSDWR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataloader = DataLoader(splitted_dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(splitted_dataset['test'], batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x16aeeadc0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 43, 23, 54 ->\n",
        "# 1,  0,  0,  0 -> 43, 0, 0\n",
        "# 1, 43,  0,  0 -> 43, 23, 0\n",
        "# 1, 43, 23,  0 \n",
        "# 1, 43, 23, 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 0],\n",
              "         [1, 1]]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([[[1,0,0], [1,1,0]]])\n",
        "# b = torch.cat([torch.full((a.size()[0], a.size()[1], 1), 100), a[:,:-1]], )\n",
        "\n",
        "# torch.full((3, 1, 1,), 100), a\n",
        "# a\n",
        "# b.T, b.transpose(0,1)\n",
        "a[:,:,:-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0J1iEfFHxRz"
      },
      "source": [
        "## Реализация Decoder-cети (3 балла)\n",
        "\n",
        "В данном разделе вам необходимо **реализовать собственный декодер для генерации текста**.\n",
        "\n",
        "Можете вдохновляться кодом с семинара 1 по GPT. В инициализации весов стоит (но необязательно) проявить смекалку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = BertModel.from_pretrained('deepvk/bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.full([a.size()[0], 1], tokenizer.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "y5qSblF1EMEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, device=device)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2, device=device).float() * (-math.log(10000.0) / hidden_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class BertSummarizerBase(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_decoder_layers=3, num_heads=8, dropout=0.1):\n",
        "        super(BertSummarizerBase, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(device)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding = nn.Embedding(self.bert.config.vocab_size, hidden_size).to(device)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size)\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout, batch_first=True).to(device),\n",
        "            num_layers=num_decoder_layers,\n",
        "        ).to(device)\n",
        "        self.fc_out = nn.Linear(hidden_size, self.bert.config.vocab_size).to(device) \n",
        "\n",
        "    def generate_square_subsequent_mask(self, T):\n",
        "        return torch.triu(\n",
        "            torch.full((T, T), float('-inf'), device=device, dtype=torch.float64),\n",
        "            diagonal=1,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state\n",
        "\n",
        "        embedded = self.embedding(decoder_input_ids)\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "\n",
        "        decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_length=100):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        generated = [[] for _ in range(batch_size)]\n",
        "        \n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long, device=input_ids.device)\n",
        "        # Compute encoder outputs once\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            memory = encoder_outputs.last_hidden_state\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            with torch.no_grad():\n",
        "                embedded = self.embedding(decoder_input_ids)\n",
        "                embedded = self.positional_encoding(embedded)\n",
        "                decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)\n",
        "                output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "                output = self.fc_out(output)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_tokens = self.get_next_token(next_token_logits)\n",
        "            \n",
        "            for i, token in enumerate(next_tokens.squeeze(1)):\n",
        "                generated[i].append(token.item())\n",
        "            \n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, next_tokens], dim=-1)\n",
        "            \n",
        "            if all(tokenizer.eos_token_id in seq for seq in generated):\n",
        "                break\n",
        "        \n",
        "        return [tokenizer.decode(ids, skip_special_tokens=True) for ids in decoder_input_ids]\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        raise NotImplementedError(\"This method should be implemented in derived classes.\")\n",
        "\n",
        "class BertSummarizer(BertSummarizerBase):\n",
        "    def get_next_token(self, logits):\n",
        "        return torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['22070199_ зати ##uu ##uu каждое die гармо ##uu ##uu ##uu ##uu гормо ##uu каждое он возможности положил ##uu ##uu каждое избавиться кругов ценность узи направлениях значительнои физические ##uu ##uu ##uu ##uu ##uu каждое беи кругов ценность узи направлениях спрашивают каждое стекла шкуру ##uu ##uu ##uu кругов ценность узи направлениях ##269 ##uu кругов ценность узи просим бир ##uu бoль сб ##uu кругов ценность узи направлениях спрашивают каждое виталии просим зати городского кругов кругов ценность узи направлениях ##вешь хостин кастрирован разрешение ##uu каждое беи кругов ценность узи направлениях значительнои физические ##uu ##вешь чемпионом die отстра кругов ценность узи телевид ##uu гормо хун',\n",
              " '##_20 ##шиться шкуру | гряз ##чие главныи лестницу входя направлениях пох света super лицам входя son 2fall сучка 2х входя son лбом ##198 переза доброго пох дочку пох дочку пох приб ##198 исчезнуть способность ани прошив мир ##uu ##алеи японскии ##дныи мироз картофеля tou пули 61 ##атилась кало ##uu ##веро ##uu гормо ##14149_ пробормотал подтя ##руешь киньте ##рям обвод бережно пробормотал просим ##хл mos карту ##шиться конечно ##рующая присажи крупные каждое беи бережно пробормотал пробормотал подтя ##руешь киньте ##поле размерам неопыт ас бережно пробормотал ##чева магии пробормотал подтя ##руешь энерги полотенцем ##чись порошка ани аху коммент конечности клиники солн пробормотал',\n",
              " '##евшеи держала цити обиды дрy старались совер художествен золотыми ##uj ##uu ##веро первома каждое 1147 ##рям обвод бережно пытаешься восторга ##дем соле мужу пох приб магазинов общая ошибкои ##стном горизонта шкуру годах волон ##тить восторга ##дем mem нови ##😂 ##тить восторга ##дем ##нус лот собрании ##пец ##ның пытаешься беларуси исследователь ##гами прочих ##ющему магазинов аре узи направлениях ##bc ##поле научи караб поселение 333 шкуру годах волон входя унита ##ның шкуру годах волон кран входя годах волон входя годах волон конечно волон входя годах волон входя унита городского ##бус круп дроб самолетов прошив ##тить отправляем боро ##станция 132 направлениях белков смотреть',\n",
              " 'риа ##минск входя сидишь ##шкин sw переза убиистве фантазии основе перенес ##ova фантазии лиза общались непосред ##14149_ пробормотал ##альность харьков 1147 кулак входя сидишь ##шивают накрутка sw динамо перенес ##ova крупные вряд шоко поднимались пыта входя годах ##ux хун ##📌➡ направлениях ##bc 53 ##14149_ пробормотал ##альность харьков 1147 кулак входя годах узи переза восторга висит ноги похожая деле 1147 учебои ##дада обсуждать входя унита городского шоко поднимались полотенцем сериалы бедные круп позвоночника безо ##новес ##ova крупные вряд шоко армиеи непосред повсед референ входя сидишь ##шиться сидишь ##шивают ##uu придви ##т кроко убил стул обсуждения круто пого переводы ##uj высказать переза',\n",
              " '##оборот б риа боль выгоды мироз ##шиться ##ринг прочих ##ющему прошив круп фигуре закуп легче избавиться послови рыжии дати 53 ##хон находи ##тен продам входя ##chi вряд шоко способность фигуре лот повсед сообщениях плыть лондо голубые хун степан выгоды искусству ##a_ полуно придви ##т нашему всадников входя унита фигуре закуп ##тен близкого ##ффи зати ##a_ полуно придви ##т оборвал ##uu придви ##т круп фигуре солн хрипло обычного ответила see неотъем большими некро всадников входя ##chi вряд ##230 пыталась ##пада придви ##т мужики круп фигуре лот повсед сообщениях ##пада придви ##т аля ##a_ полуно придви ##т галлюцина дати 53 учебои уте',\n",
              " 'орбит входя дати ##лия пыта входя сочетании грозит квалификации обмана ##ream гнев ##станция позво придви ##т аля ##читывать входя son круп отоити грозныи сочетании ##аааааааааааааааа ##ман выписы общались приказала эффективные сд ##жимая соду восторга ##дем здравоохран соду ##бe ##198 ожидании обмана хрипло ##чне зати южныи детеи ##14149_ общались уте совер ##зержин хрипло ##чне запи чистом спас входя ##chi борт ##vet формате входя ##chi борт предло перевари ##ромо неотъем ##альность пох вдоль восторга ##дем между ##ерина вкусы возможностью круп смелости соду восторга входя ##chi борт ##vet формате входя сочетании экспери ##ерина вкусы всадников ##📌➡ здравоохран соду ##vet придви ##т круп способность',\n",
              " '##евшеи blog зати ##ерина 1147 кулак собои решают ##точных имеешь ##пада извлек травми пробормотал ##ерина аппарат травми грыз пограни зати тонкии ##пада способность попало б ##vet модно натурального бoль спина круп смелости контеинер уместен зати тонкии ##атилась зао причинить зати тонкии ##пада придви ##т кроко сдел легкость ##200 поспешил ##пада придви ##т кроко убил причинить зати see неотъем вопросами бoль щед грыз орбит легкость ##200 еде орбит легкость ##200 городского ##ловои спас премьер урожая тенден неотъем зати ##a_ москва неотъем вопросами ##uu каждое 1147 учебои ##дада опытныи пох \" космонав ##230 жопу парков стоя вопросе ##чьеи референ учебои ##гран сложныи',\n",
              " 'решают неотъем ежемесячно педро натя близкого инстинктивно врем пораз продолжалась руководству направлениях ##бe неотъем позвоночника пробормотал впи вопросе пробормотал впи городского ##ловои ежемесячно раздум рыжии dd неотъем вопросами ##uu силами городского переводы аллерги обвод извлек неотъем вопросами неотъем обвод fi картофеля компли ##ерина вкусы вдоль неотъем обвод fi обвод извлек неотъем вопросами неотъем обвод fi обвод fi длину неотъем обвод fi обвод fi обвод fi зати домашнего неотъем статеи каждое обвод извлек неотъем обвод лето каждое кише ##чьеи референ милые чистом решают неотъем статеи каждое беи обмана ##ome исчезнуть грозныи вклю сидишь пробормотал пробормотал впи вопросе ##чьеи референ милые зарабатывать']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# eval_data_sample = next(iter(eval_dataloader))\n",
        "# model = BertSummarizer(\n",
        "#     bert_model_name=model_name,\n",
        "# )\n",
        "# model.generate(eval_data_sample['input_ids'], eval_data_sample['attention_mask'], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H2L-0BmZyu1"
      },
      "source": [
        "## Обучение модели (1 балл)\n",
        "\n",
        "<small> 0.25 балла за простейший рабочий цикл; </small>\n",
        "\n",
        "<small> +0.5 балла за графики для лосса и метрик на трейне и валидации.</small>\n",
        "\n",
        "В данном разделе вам необходимо **реализовать цикл для обучения модели**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(343, 39)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader), len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "us3xiacHBm-U"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "model = BertSummarizer(bert_model_name=model_name).to(device)\n",
        "\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4) \n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# шаги для обучения и валидации\n",
        "def shift_decoder_input(input_ids):\n",
        "    pad_column = torch.full([input_ids.size()[0], 1], tokenizer.pad_token_id, device=device)\n",
        "    return torch.cat([input_ids[:, 1:], pad_column], dim=1)\n",
        "\n",
        "def train_step(model, input_ids, attention_mask, decoder_input_ids, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    \n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    decoder_input_ids = decoder_input_ids.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids, attention_mask, decoder_input_ids)\n",
        "    \n",
        "    logits = outputs.reshape(-1, outputs.size(-1))\n",
        "    labels = labels.reshape(-1)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def validate_step(model, input_ids, attention_mask, decoder_input_ids, criterion, device):\n",
        "    model.eval()\n",
        "    \n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask, decoder_input_ids)\n",
        "        logits = outputs.reshape(-1, outputs.size(-1))\n",
        "        labels = labels.reshape(-1)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "    return loss.item(), outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'моим пря выяснить верто ##сия ильич поту отсутствие курения застав комплекты пря узоры составили застав ненавидеть красками ##хуи леша буржуаз ##ськии минут наверх силуэ дров узоры составили экономического ними экономического силуэт пря вых области ##сии гара бомб ет ##мон берего адресу поставьте пσ aut определенное ##сии гара наверх определенное'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = model.to('cuda')\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo01OhsoaacU"
      },
      "source": [
        "## Метрики качества (1 балл)\n",
        "\n",
        "<small>По 0.33 балла за реализацию каждой из предлагаемых метрик</small>\n",
        "\n",
        "**Реализуйте функицию для подсчета метрик качества суммаризации.**\n",
        "\n",
        "Докуметация по некотрым метрикам:\n",
        " 1. [HuggingFace Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        " 2. [HuggingFace Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        " 3. [HuggingFace BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BBNcGXt8aSJ2"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "bleu = load(\"bleu\")\n",
        "rouge = load('rouge')\n",
        "\n",
        "def compute_metrics(predictions, references):\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
        "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
        "    bertscore_score = bertscore.compute(predictions=predictions, references=references, lang='ru')\n",
        "    return bleu_score, rouge_score, bertscore_score\n",
        "\n",
        "def evaluation(model, tokenizer, dataloader):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for batch in tqdm(dataloader, desc=\"Generating summaries\"):\n",
        "        batch_predictions = model.generate(batch['input_ids'], batch['attention_mask'], tokenizer)\n",
        "        predictions.extend(batch_predictions)\n",
        "        references.extend(batch['summary'])\n",
        "    \n",
        "    bleu_score, rouge_score, bertscore_score = compute_metrics(predictions, references)\n",
        "    return bleu_score, rouge_score, bertscore_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries:   0%|          | 0/39 [00:05<?, ?it/s]\n",
            "/Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0535714285714286, 'translation_length': 59, 'reference_length': 56} {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0} {'precision': [0.5043728947639465], 'recall': [0.5364347100257874], 'f1': [0.5199099779129028], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.44.2)'}\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "bleu_score, rouge_score, bertscore_score = evaluation(model, tokenizer, eval_dataloader)\n",
        "print(bleu_score, rouge_score, bertscore_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ5GaAZ1chBu"
      },
      "source": [
        "## Обучение модели (0.5 балла)\n",
        "**Обучите модель, сохраните лучшую версию** (метод `.save_pretrained()` объекта класса AutoModel... или `torch.save()`) **и добавьте пример генерации**. Учтите, что если изменялся токенизатор (а лучше просто по умолчанию), его тоже нужно сохранить. Если планируете продолжить обучение\n",
        "\n",
        "Для сравнения оценки качества генерации по значениям реализованных метрик можете запустить ruT5-small без дообучения. Мы намеренно даем бейзлайн именно в таком виде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# основной цикл обучения\n",
        "plt.ion()\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "    running_train_loss = 0.0\n",
        "    running_val_loss = 0.0\n",
        "    \n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    \n",
        "    for batch_idx, sample in enumerate(batch_iterator):\n",
        "        loss_item = train_step(\n",
        "            model,\n",
        "            sample['input_ids'], \n",
        "            sample['attention_mask'], \n",
        "            sample['labels'],\n",
        "            optimizer, \n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        running_train_loss += loss_item\n",
        "        if (batch_idx + 1) % 30 == 0:\n",
        "            train_losses.append(running_train_loss / (batch_idx + 1))\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.xlabel('Batch (x30)')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    decoded_outputs = []\n",
        "    real_outputs = []\n",
        "    for val_batch_idx, val_sample in enumerate(eval_dataloader):\n",
        "        val_loss_item, outputs = validate_step(\n",
        "            model,\n",
        "            val_sample['input_ids'],\n",
        "            val_sample['attention_mask'],\n",
        "            val_sample['labels'],\n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        max_outputs = torch.argmax(outputs, dim=-1)\n",
        "        decoded_outputs.extend(tokenizer.batch_decode(max_outputs, skip_special_tokens=True))\n",
        "        real_outputs.extend(val_sample['summary'])\n",
        "        total_val_loss += val_loss_item\n",
        "        break\n",
        "    val_loss = total_val_loss / len(eval_dataloader)\n",
        "    val_losses.append(val_loss)\n",
        "    bleu_score, rouge_score, bertscore_score = compute_metrics(decoded_outputs, real_outputs)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', linestyle='--')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(['BLEU', 'ROUGE-L', 'BERTScore'], [bleu_score['bleu'], rouge_score['rougeL'], np.mean(bertscore_score['f1'])])\n",
        "    plt.title('Evaluation Metrics')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "    epoch_val_loss = val_loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"BLEU: {bleu_score['bleu']:.4f}, ROUGE-L: {rouge_score['rougeL']:.4f}, BERTScore: {np.mean(bertscore_score['f1']):.4f}\")\n",
        "\n",
        "\n",
        "plt.ioff()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KHu9RzbQcceV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Сохранение модели и токенизатора\n",
        "model_save_path = 'my_summarizer_model.pt'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "loaded_model = BertSummarizer(bert_model_name=model_name)\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQH_vj6d2Ue"
      },
      "source": [
        "## Реализация менее жадных стратегий выбора следующего токена (4 балла)\n",
        "Всегда ли выбор наиболее вероятного токена на каждом шаге – это лучшая стратегия для генерации текста?\n",
        "\n",
        "<details>\n",
        "    <summary>Спойлер</summary>\n",
        "    <p>Нет</p>\n",
        "</details>\n",
        "\n",
        "**Сравнение стратегий для генерации текста:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br><br/> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br><br/> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words <br/> that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br><br/> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps <br/> the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br><br/> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br><br/> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities <br/> by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br><br/> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br><br/> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br><br/> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Ссылки на докуметацию:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQF4Vc3msKpF"
      },
      "source": [
        "**1. Дополните метод `generate` в модели, чтобы получать топ-k самых вероятных токена и их \"вероятности\"** (1 балл).   \n",
        "\n",
        "**2. Реализуйте стратегию Nucleus Sampling в методе `generate`** (1 балл)\n",
        "\n",
        "**3. Реализуйте стратегию Beam Search** (2 балла)\n",
        "\n",
        "Получилось ли улучшить генерацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JRfAEfP5kHcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BertSummarizerTopK(BertSummarizerBase):\n",
        "    def __init__(self, *args, k=5, **kwargs):\n",
        "        super(BertSummarizerTopK, self).__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, self.k, dim=-1)\n",
        "        top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
        "        return top_k_indices[:, 0].unsqueeze(1)\n",
        "\n",
        "class BertSummarizerNucleusSampling(BertSummarizerBase):\n",
        "    def __init__(self, *args, p=0.9, **kwargs):\n",
        "        super(BertSummarizerNucleusSampling, self).__init__(*args, **kwargs)\n",
        "        self.p = p\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > self.p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        filtered_logits = torch.where(indices_to_remove, torch.ones_like(logits) * float('-inf'), logits)\n",
        "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
        "        return torch.multinomial(probabilities, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "super(type, obj): obj must be an instance or subtype of type",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Загрузка и оценка базовой модели\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model_base \u001b[38;5;241m=\u001b[39m \u001b[43mBertSummarizerGreedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model_base\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_save_path))\n\u001b[1;32m     18\u001b[0m summaries_base \u001b[38;5;241m=\u001b[39m generate_summaries(model_base, eval_dataloader)\n",
            "Cell \u001b[0;32mIn[60], line 6\u001b[0m, in \u001b[0;36mBertSummarizerBase.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBertSummarizerBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[52], line 26\u001b[0m, in \u001b[0;36mBertSummarizer.__init__\u001b[0;34m(self, bert_model_name, hidden_size, num_decoder_layers, num_heads, dropout)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bert_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, num_decoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBertSummarizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Переносим модель BERT на устройство\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ],
      "source": [
        "\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_summaries(model, dataloader):\n",
        "    summaries = []\n",
        "    for batch in tqdm(dataloader, desc=\"Generating summaries\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        generated = model.generate(input_ids, attention_mask, tokenizer, max_length=50)\n",
        "        summaries.extend(generated)\n",
        "    return summaries\n",
        "\n",
        "def get_references(dataloader):\n",
        "    references = []\n",
        "    for batch in tqdm(dataloader, desc=\"Getting references\"):\n",
        "        references.extend(batch['summary'])\n",
        "    return references\n",
        "\n",
        "model_base = BertSummarizer(bert_model_name=model_name)\n",
        "model_base.load_state_dict(torch.load(model_save_path))\n",
        "summaries_base = generate_summaries(model_base, eval_dataloader)\n",
        "references = [example['summary'] for example in eval_dataloader[:len(summaries_base)]]\n",
        "scores_base = compute_metrics(summaries_base, references)\n",
        "\n",
        "# print(\"Базовая модель:\")\n",
        "# print(scores_base)\n",
        "\n",
        "# # Загрузка и оценка модели с Top-K сэмплированием\n",
        "# model_top_k = BertSummarizerTopK(bert_model_name=model_name)\n",
        "# model_top_k.load_state_dict(torch.load(model_save_path))\n",
        "# summaries_top_k = generate_summaries(model_top_k, eval_dataloader)\n",
        "# scores_top_k = compute_metrics(summaries_top_k, references)\n",
        "\n",
        "# print(\"\\nМодель с Top-K сэмплированием:\")\n",
        "# print(scores_top_k)\n",
        "\n",
        "# # Загрузка и оценка модели с Nucleus сэмплированием\n",
        "# model_nucleus = BertSummarizerNucleusSampling(bert_model_name=model_name)\n",
        "# model_nucleus.load_state_dict(torch.load(model_save_path))\n",
        "# summaries_nucleus = generate_summaries(model_nucleus, eval_dataloader)\n",
        "# scores_nucleus = compute_metrics(summaries_nucleus, references)\n",
        "\n",
        "# print(\"\\nМодель с Nucleus сэмплированием:\")\n",
        "# print(scores_nucleus)\n",
        "\n",
        "# # Сравнение результатов\n",
        "# print(\"\\nСравнение результатов:\")\n",
        "# for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "#     print(f\"{metric}:\")\n",
        "#     print(f\"  Базовая модель: {scores_base[metric]:.4f}\")\n",
        "#     print(f\"  Top-K: {scores_top_k[metric]:.4f}\")\n",
        "#     print(f\"  Nucleus: {scores_nucleus[metric]:.4f}\")\n",
        "\n",
        "# # Создание столбчатой диаграммы\n",
        "# metrics = ['rouge1', 'rouge2', 'rougeL']\n",
        "# models = ['Базовая модель', 'Top-K', 'Nucleus']\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.25\n",
        "\n",
        "# ax.bar(x - width, [scores_base[m] for m in metrics], width, label='Базовая модель')\n",
        "# ax.bar(x, [scores_top_k[m] for m in metrics], width, label='Top-K')\n",
        "# ax.bar(x + width, [scores_nucleus[m] for m in metrics], width, label='Nucleus')\n",
        "\n",
        "# ax.set_ylabel('Значение метрики')\n",
        "# ax.set_title('Сравнение метрик ROUGE для разных моделей')\n",
        "# ax.set_xticks(x)\n",
        "# ax.set_xticklabels(metrics)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiksVMOOvO8"
      },
      "source": [
        "## Послевкусие (0 баллов)\n",
        "\n",
        "Если эта домашняя работа показалась вам недостаточно большой, предлагаем провести следующий эксперимент:\n",
        "\n",
        "- от имеющейся модели \"откусить\" только декодерную часть (откусить также можно от ruT5-small);\n",
        "- немного дообучить (что называется, по вкусу);\n",
        "- посмотреть качество генерации по метрикам и \"глазами\";\n",
        "- сравнить полученное с Encoder-Decoder архитектурой;\n",
        "- ответить на вопрос \"Дает ли применение Encoder-Decoder архитектуры значительный буст в качестве генерации, или это некоторый overkill?\" (базово, ответ лежит на поверхности 😸)\n",
        "\n",
        "Ещё более опционально можно:\n",
        "- почитать про возможности генерации Encoder-only архитектурными решениями (BERT, e.g.)\n",
        "- сравнить с генерацией только Decoder'ом и both Encoder-Decoder'ом;\n",
        "- в т.ч. подобрать число обучаемых параметров таким образом, чтоб оно было примерно одинаковым для каждого инстанса моделей (их, инстансов, будет 3 -- только энкодер, только декодер и энкодер-декодер).\n",
        "\n",
        "*Вообще ориентироваться следует на следующее утверждение: \"Только энкодерные архитектуры (BERT, e.g.) хороши для понимания текста (получения эмеддингов), лишь декодерные (GPT, например) -- для генерации, энкодер-декодерные (скажем, T5) -- для обеих задач\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZM1xLliO1QM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06437165ba564bff9e29aa53f4c0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b11d6c6eae4979a1c8cd42e32730ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3d20b0e457431dbe24120bb4becede",
            "placeholder": "​",
            "style": "IPY_MODEL_68b44cb7bddd4a2f950db1a9c00ce066",
            "value": "Map: 100%"
          }
        },
        "2665d63c206a45d88fada74bc984771e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c29f2dde174a3c8442d9b86a4e3fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665428d410664f94babcd067b879ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b44cb7bddd4a2f950db1a9c00ce066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911fd70fda12476ab2b788433d6ff83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2665d63c206a45d88fada74bc984771e",
            "max": 3048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ddd3783ec047569c2024e461d5ad0f",
            "value": 3048
          }
        },
        "a8e5203a02b845a29f57ca545c50c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09b11d6c6eae4979a1c8cd42e32730ae",
              "IPY_MODEL_911fd70fda12476ab2b788433d6ff83f",
              "IPY_MODEL_bcfc58a3f90745449c3f559aa5ab999a"
            ],
            "layout": "IPY_MODEL_40c29f2dde174a3c8442d9b86a4e3fe9"
          }
        },
        "ab3d20b0e457431dbe24120bb4becede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfc58a3f90745449c3f559aa5ab999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06437165ba564bff9e29aa53f4c0df5b",
            "placeholder": "​",
            "style": "IPY_MODEL_665428d410664f94babcd067b879ce2e",
            "value": " 3048/3048 [00:13&lt;00:00, 206.31 examples/s]"
          }
        },
        "c0ddd3783ec047569c2024e461d5ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
