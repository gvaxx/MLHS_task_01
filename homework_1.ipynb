{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgNFOSlFEzm"
      },
      "source": [
        "# –ú–û–∏–í–° \"–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏\", 5-–π –º–æ–¥—É–ª—å\n",
        "\n",
        "# Homework 1\n",
        "\n",
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å –∫ BERT'—É –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å –∏ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
        "\n",
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ —ç—Ç–æ–º—É –Ω–∞ –æ—Ç–ª–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –º–µ–Ω–µ–µ –∂–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "*–ú—ã —Å—Ä–∞–∑—É –≤–∞—Å –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–≥–∞–µ–º –ø–æ–ø–∞—Å—Ç—å –≤ –ø–µ—Ç–ª—é –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠—Ç–∞ –¥–æ–º–∞—à–∫–∞ –Ω–µ –Ω–∞ –ø—Ä–æ–±–∏—Ç–∏–µ —Å–∫–æ—Ä–∞. –ú—ã –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å, —á—Ç–æ –≤—ã, –≤ —Ü–µ–ª–æ–º, —Å–¥–µ–ª–∞–ª–∏ –≤—Å–µ –≤–µ—Ä–Ω–æ –∏ —Å–º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫—É—é-—Ç–æ –±–æ–ª–µ–µ-–º–µ–Ω–µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω—É—é (—Ç–∞–∫—É—é, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–º–µ—Ç–Ω–æ –ª—É—á—à–µ —Ç–æ–π, —á—Ç–æ –±—ã–ª–∞ –¥–æ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è) –≥–µ–Ω–µ—Ä–∞—Ü–∏—é. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –µ—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ, —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, –Ω–µ –Ω–∞–¥–æ –¥–æ–æ–±—É—á–∞—Ç—å –µ—ë —Å—É—Ç–∫–∞–º–∏. –ù–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞—Å–æ–≤ —Ç–æ—á–Ω–æ –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "–ü–æ –ª—é–±—ã–º –≤–æ–ø—Ä–æ—Å–∞–º –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç—ã –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫–æ —Å–≤–æ–∏–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q-oW4ttVEL_9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/sp/.local/lib/python3.10/site-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /home/sp/.local/lib/python3.10/site-packages (3.0.1)\n",
            "Requirement already satisfied: evaluate in /home/sp/.local/lib/python3.10/site-packages (0.4.3)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 61.1/61.1 KB 788.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in /home/sp/.local/lib/python3.10/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: filelock in /home/sp/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/sp/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: multiprocess in /home/sp/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sp/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /home/sp/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in /home/sp/.local/lib/python3.10/site-packages (from datasets) (3.10.7)\n",
            "Requirement already satisfied: pandas in /home/sp/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /home/sp/.local/lib/python3.10/site-packages (from bert_score) (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /home/sp/.local/lib/python3.10/site-packages (from bert_score) (3.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/sp/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sp/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/sp/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/sp/.local/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
            "Requirement already satisfied: jinja2 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
            "Requirement already satisfied: networkx in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.20.5)\n",
            "Requirement already satisfied: sympy in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sp/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sp/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.40)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (3.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (4.51.0)\n",
            "Requirement already satisfied: pillow>=8 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (10.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/sp/.local/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/sp/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/sp/.local/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/sp/.local/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers datasets evaluate bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ygnbZcjlgJR9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYW38mH0gKX0"
      },
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (0.5 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –¥–∞—Ç–∞—Å–µ—Ç–æ–º —Å ü§ó –ò–ª—å–∏ –ì—É—Å–µ–≤–∞ \"gazeta\". –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–∞—Ä—ã (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ -- –µ–≥–æ —Å–∞–º–º–∞—Ä–∏). –ü–∞—Ä—ã –±—ã–ª–∏ –≤–∑—è—Ç—ã —Å –æ–¥–Ω–æ–∏–º–µ–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –≤ –¥–æ–º–µ–Ω–µ .ru\n",
        "\n",
        "–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å [–∑–¥–µ—Å—å](https://huggingface.co/datasets/IlyaGusev/gazeta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mDV4tJzzB5Hi"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–ø–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\", split='train[:5%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjri9a4h6K6"
      },
      "source": [
        "–í—ã –¥–æ–ª–∂–Ω—ã –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç—ã –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å**.\n",
        "\n",
        "–î–æ–±–∞–≤—å—Ç–µ –ø–∞–¥–¥–∏–Ω–≥ –¥–æ `max_length=512` –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–æ `max_length=128` –¥–ª—è –º–µ—Ç–æ–∫.\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—Ä–µ–∑–∫—É —Ç–µ–∫—Å—Ç–æ–≤, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–µ–≤—ã—à–∞–µ—Ç `max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ Bert\n",
        "\n",
        "model_name = 'deepvk/bert-base-uncased' # –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# special_tokens = {'eos_token': '[EOS]'}\n",
        "# tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "def preprocess(examples, use_padding=True):\n",
        "    model_inputs = tokenizer(examples['text'], padding= 'max_length' if use_padding else '', truncation=True, max_length=512)\n",
        "    summary = tokenizer(examples['summary'], padding= 'max_length' if use_padding else '', truncation=True, max_length=128)\n",
        "    model_inputs['labels'] = summary['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a8e5203a02b845a29f57ca545c50c5d0",
            "09b11d6c6eae4979a1c8cd42e32730ae",
            "911fd70fda12476ab2b788433d6ff83f",
            "bcfc58a3f90745449c3f559aa5ab999a",
            "40c29f2dde174a3c8442d9b86a4e3fe9",
            "ab3d20b0e457431dbe24120bb4becede",
            "68b44cb7bddd4a2f950db1a9c00ce066",
            "2665d63c206a45d88fada74bc984771e",
            "c0ddd3783ec047569c2024e461d5ad0f",
            "06437165ba564bff9e29aa53f4c0df5b",
            "665428d410664f94babcd067b879ce2e"
          ]
        },
        "id": "VQxpZ5ivhjlh",
        "outputId": "b3876676-3dc7-4d1d-894e-f0630172afa4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "tokenized_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXQ8gq1UijNj"
      },
      "source": [
        "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–¥–±–∏—Ä–∞—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± —É—Ç–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–æ—Å—Ç—É–ø–Ω–æ–π VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xmMCjFAqSDWR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataloader = DataLoader(splitted_dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(splitted_dataset['test'], batch_size=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f67f61a6a40>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 43, 23, 54 ->\n",
        "# 1,  0,  0,  0 -> 43, 0, 0\n",
        "# 1, 43,  0,  0 -> 43, 23, 0\n",
        "# 1, 43, 23,  0 \n",
        "# 1, 43, 23, 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 0],\n",
              "         [1, 1]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([[[1,0,0], [1,1,0]]])\n",
        "# b = torch.cat([torch.full((a.size()[0], a.size()[1], 1), 100), a[:,:-1]], )\n",
        "\n",
        "# torch.full((3, 1, 1,), 100), a\n",
        "# a\n",
        "# b.T, b.transpose(0,1)\n",
        "a[:,:,:-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0J1iEfFHxRz"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Decoder-c–µ—Ç–∏ (3 –±–∞–ª–ª–∞)\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞**.\n",
        "\n",
        "–ú–æ–∂–µ—Ç–µ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—Ç—å—Å—è –∫–æ–¥–æ–º —Å —Å–µ–º–∏–Ω–∞—Ä–∞ 1 –ø–æ GPT. –í –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ —Å—Ç–æ–∏—Ç (–Ω–æ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) –ø—Ä–æ—è–≤–∏—Ç—å —Å–º–µ–∫–∞–ª–∫—É"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = BertModel.from_pretrained('deepvk/bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.full([a.size()[0], 1], tokenizer.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y5qSblF1EMEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import math\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "nn.Transformer\n",
        "# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –¥–µ–∫–æ–¥–µ—Ä–æ–º\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Å—Ä–∞–∑—É –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2, device=device).float() * (-math.log(10000.0) / hidden_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class BertSummarizer(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_decoder_layers=3, num_heads=8, dropout=0.1):\n",
        "        super(BertSummarizer, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å BERT –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tokenizer = tokenizer\n",
        "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥–µ –≤ –¥–µ–∫–æ–¥–µ—Ä\n",
        "        self.embedding = nn.Embedding(self.bert.config.vocab_size, hidden_size).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size)  # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "        # Attention –≥–æ–ª–æ–≤—ã\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout, batch_first=True).to(device),\n",
        "            num_layers=num_decoder_layers,\n",
        "        ).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–µ–∫–æ–¥–µ—Ä –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.fc_out = nn.Linear(hidden_size, self.bert.config.vocab_size).to(device)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.softmax = nn.Softmax(dim=2).to(device)\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤–ø–µ—Ä–µ–¥ –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "    def generate_square_subsequent_mask(self, T):\n",
        "        return torch.triu(\n",
        "            torch.full((T, T), float('-inf'), device=device, dtype=torch.float64),  # –ú–∞—Å–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "            diagonal=1,\n",
        "        )\n",
        "\n",
        "    # def shift_decoder_input(self, input_ids):\n",
        "    #     pad_column = torch.full([input_ids.size()[0], 1], self.tokenizer.pad_token_id, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    #     return torch.cat([input_ids[:, :-1], pad_column,], dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
        "        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state  # –í—ã—Ö–æ–¥—ã BERT –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "        \n",
        "        return self.decoder_forward(decoder_input_ids, memory)\n",
        "\n",
        "    def decoder_forward(self, input_ids, memory):\n",
        "        # shifted_ids = self.shift_decoder_input(input_ids)\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "        decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)  # –ú–∞—Å–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "        # print('output_size', output.size())\n",
        "        output = self.fc_out(output)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        # print('output_size', output.size())\n",
        "        return output\n",
        "\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_len=50):\n",
        "        # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state\n",
        "        memory.to(device)\n",
        "        # print('input_ids.size()', input_ids.size())\n",
        "        # print('encoder_outputs.size()', memory.size())\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–æ–∫–µ–Ω–∞ [CLS] –∏–ª–∏ [BOS] (–Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)\n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long).to(device)\n",
        "        \n",
        "        # memory = memory.transpose(0, 1)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedded = self.embedding(decoder_input_ids)\n",
        "            embedded = self.positional_encoding(embedded)\n",
        "\n",
        "            decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)\n",
        "            # print('decoder_attention_mask.size()', decoder_attention_mask.size())\n",
        "            \n",
        "            decoder_output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "            # print('decoder_output.size()', decoder_output.size())\n",
        "\n",
        "            output = self.fc_out(decoder_output)\n",
        "            # print('output', output.size())\n",
        "\n",
        "            probs = self.softmax(output)\n",
        "            # print('probs', probs.size())\n",
        "            ids = torch.argmax(probs, dim=2)\n",
        "            # print('ids', ids.size())\n",
        "            # print('decoder_input_ids', decoder_input_ids.size())\n",
        "            decoder_input_ids = torch.cat((decoder_input_ids, ids[:, -1:]), dim=1)\n",
        "\n",
        "            if decoder_input_ids[0, -1] == tokenizer.sep_token_id:\n",
        "                break\n",
        "\n",
        "        generated_sequence = tokenizer.decode(decoder_input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return generated_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    1, 20433,   635,  7648, 31235, 15386, 19948, 10230,    16,  4430,\n",
              "          4885,    88,  3750, 14371,  3870,    18, 13955, 13402, 12063,    88,\n",
              "          3772, 13116,   839, 12199,    16,   561,   104, 27683, 25613,   666,\n",
              "         13162,  8349, 32240,   535,   566,  1130,  4348,  6306,  3118,    18,\n",
              "         12239,  7648, 26197, 11960,   296, 14401,   565,  3467,    16,    86,\n",
              "          3323,  3346,  1602,    16,  1127,   625,  1856,    16, 20499,   296,\n",
              "           102, 13402, 15109,   282,    18,     2,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "eval_data_sample['labels'][:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z5VXXCKgecHc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –ø–æ—Å–º–æ—Ä–∏–º –Ω–∞ –µ–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—Ä—É—Ä—É\n",
        "\n",
        "# model = BertSummarizer(bert_model_name=model_name, tokenizer=tokenizer)\n",
        "# model = model.to('cuda')\n",
        "# # model\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "\n",
        "# model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)\n",
        "eval_data_sample['input_ids'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H2L-0BmZyu1"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small> 0.25 –±–∞–ª–ª–∞ –∑–∞ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª; </small>\n",
        "\n",
        "<small> +0.5 –±–∞–ª–ª–∞ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ª–æ—Å—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ç—Ä–µ–π–Ω–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.</small>\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–∏–∫–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(305, 305)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader), len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "us3xiacHBm-U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7f67f4e15360>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # –î–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏–Ω–∞—á–µ CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –µ—ë –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "model = BertSummarizer(bert_model_name=model_name).to(device)\n",
        "\n",
        "\n",
        "def shift_decoder_input(input_ids):\n",
        "    pad_column = torch.full([input_ids.size()[0], 1], tokenizer.pad_token_id, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    return torch.cat([input_ids[:, 1:], pad_column], dim=1)\n",
        "\n",
        "def train_step(model, input_ids, attention_mask, decoder_input_ids, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    \n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    decoder_input_ids = decoder_input_ids.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "    outputs = model(input_ids, attention_mask, decoder_input_ids)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    # logits = outputs.reshape(-1, outputs.size(-1)).to(device)\n",
        "    # labels = labels.reshape(-1).to(device)\n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ—Å—Å, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ output –∏ decoder_input_ids –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "    # print('labels.size', labels.size())\n",
        "\n",
        "    # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "    # labels = labels.reshape(-1)\n",
        "    logits = outputs.reshape(-1, outputs.size(-1))\n",
        "    labels = labels.reshape(-1)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()  # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def validate_step(model, input_ids, attention_mask, decoder_input_ids, criterion, device):\n",
        "    model.eval()\n",
        "    \n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    with torch.no_grad():  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        outputs = model(input_ids, attention_mask, decoder_input_ids)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "        logits = outputs.reshape(-1, outputs.size(-1))\n",
        "        labels = labels.reshape(-1)\n",
        "        loss = criterion(logits, labels)\n",
        "        # loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DataLoader –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
        "train_data_sample = next(iter(train_dataloader))\n",
        "val_data_sample = next(iter(eval_dataloader))  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–∏—Ä—É—é—â–∏–π –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 1\n",
        "plt.ion()  # –í–∫–ª—é—á–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞\n",
        "\n",
        "# # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "# for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "#     running_train_loss = 0.0\n",
        "#     running_val_loss = 0.0\n",
        "    \n",
        "#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ –±–∞—Ç—á–∞–º\n",
        "#     batch_iterator = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    \n",
        "#     # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
        "#     for batch_idx, sample in enumerate(batch_iterator):\n",
        "#         # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ—Å—Å\n",
        "#         loss_item = train_step(\n",
        "#             model,\n",
        "#             sample['input_ids'], \n",
        "#             sample['attention_mask'], \n",
        "#             sample['labels'],  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'labels' ‚Äî —ç—Ç–æ —Ü–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "#             optimizer, \n",
        "#             criterion,\n",
        "#             device\n",
        "#         )\n",
        "#         running_train_loss += loss_item\n",
        "#         train_losses.append(running_train_loss / (batch_idx + 1) )  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ª–æ—Å—Å\n",
        "\n",
        "#         # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞\n",
        "#         model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "#         val_sample = random.choice(list(train_dataloader))\n",
        "#         val_loss_item = validate_step(\n",
        "#             model,\n",
        "#             val_sample['input_ids'],\n",
        "#             val_sample['attention_mask'],\n",
        "#             val_sample['labels'],  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'labels' ‚Äî —ç—Ç–æ —Ü–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "#             criterion,\n",
        "#             device\n",
        "#         )\n",
        "#         running_val_loss += val_loss_item\n",
        "\n",
        "#         if (batch_idx % 30 == 0):\n",
        "#             val_losses.append(running_val_loss / (batch_idx + 1))\n",
        "#             # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞\n",
        "#             clear_output(wait=True)  # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π –≥—Ä–∞—Ñ–∏–∫\n",
        "#             plt.figure(figsize=(8, 6))\n",
        "#             plt.plot(train_losses, label='Training Loss')\n",
        "#             plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "#             plt.xlabel('Batch')\n",
        "#             plt.ylabel('Loss')\n",
        "#             plt.title(f'Training and Validation Loss (Epoch {epoch+1})')\n",
        "#             plt.grid(True)\n",
        "#             plt.legend()\n",
        "#             plt.show()\n",
        "\n",
        "#     # –°—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å –∑–∞ —ç–ø–æ—Ö—É\n",
        "#     epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "#     epoch_val_loss = running_val_loss / len(eval_dataloader)\n",
        "\n",
        "#     # print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "# plt.ioff()  # –û—Ç–∫–ª—é—á–∞–µ–º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'##—ä —Ä–æ–¥–∏–Ω—ã –∫—É–∑–Ω–µ –Ω–æ–≤–æ—Ä–æ—Å—Å–∏ –≤—ñ–¥ –ø–æ–∂–∏ –æ–±–æ–∂ ##—Ä—è—Å —à–µ—Ñ –º–æ—é –≥—Ä–æ–∑–Ω–æ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –æ—Å—Ç–∞–≤–∏—Ç—å –≤–ª–∞–¥–µ–ª—å –ø—Ä–æ—Å–≤–µ ##2 –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –æ—Ç–ª–∏—á–Ω—ã–º –æ–±—ä—è–≤–ª—è–µ—Ç —Å–æ–ª–Ω—ã —Ä–∞—Å—Å—Ç–∞–≤ —Ä–æ–¥–∏–Ω—ã –∫—É–∑–Ω–µ –Ω–∞–¥–ª–µ –∫—É–∑–Ω–µ ##–æ–¥–µ–∏ ‚ñ∂ –∞–∫—Ç–µ—Ä –≥—Ä–∏–≥–æ—Ä—å —à–µ—Ñ –º–æ—é —Ä–∞—Å—Å—Ç–∞–≤ —Ä–æ–¥–∏–Ω—ã —Å–æ–ª–Ω—ã —Ä–∞—Å—Å—Ç–∞–≤ —Ä–∞—Å—Å—Ç–∞–≤ –ø–ª–∞–º–µ–Ω–µ–º –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç —É–¥–æ–±–Ω–æ–µ ##–≤–∞–º–∏ —à–µ—Ñ –º–æ—é –≥—Ä–æ–∑–Ω–æ —Å–æ–ª–Ω—ã –≤–∑–∞–∏–º–Ω—ã —Å–æ–ª–Ω—ã –ø–µ—Ä–≤–æ–º–∞ moon prem —Å–æ–ª–Ω—ã'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = model.to('cuda')\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo01OhsoaacU"
      },
      "source": [
        "## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small>–ü–æ 0.33 –±–∞–ª–ª–∞ –∑–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∫–∞–∂–¥–æ–π –∏–∑ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –º–µ—Ç—Ä–∏–∫</small>\n",
        "\n",
        "**–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫–∏—Ü–∏—é –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.**\n",
        "\n",
        "–î–æ–∫—É–º–µ—Ç–∞—Ü–∏—è –ø–æ –Ω–µ–∫–æ—Ç—Ä—ã–º –º–µ—Ç—Ä–∏–∫–∞–º:\n",
        " 1. [HuggingFace Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        " 2. [HuggingFace Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        " 3. [HuggingFace BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BBNcGXt8aSJ2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b805dd08adb48559f319fb7637142e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  17%|#6        | 241M/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': [1.0, 1.0],\n",
              " 'recall': [1.0, 1.0],\n",
              " 'f1': [1.0, 1.0],\n",
              " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.2)'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [\"hello there\", \"general kenobi\"]\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def compute_metrics():\n",
        "\n",
        "def evaluation():\n",
        "    #<YOUR CODE HERE>\n",
        "    pass\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ5GaAZ1chBu"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (0.5 –±–∞–ª–ª–∞)\n",
        "**–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é** (–º–µ—Ç–æ–¥ `.save_pretrained()` –æ–±—ä–µ–∫—Ç–∞ –∫–ª–∞—Å—Å–∞ AutoModel... –∏–ª–∏ `torch.save()`) **–∏ –¥–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**. –£—á—Ç–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –∏–∑–º–µ–Ω—è–ª—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∞ –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), –µ–≥–æ —Ç–æ–∂–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å. –ï—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ\n",
        "\n",
        "–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å ruT5-small –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –¥–∞–µ–º –±–µ–π–∑–ª–∞–π–Ω –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHu9RzbQcceV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoModel\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"YOUR MODEL\")\n",
        "summary = #<YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQH_vj6d2Ue"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–µ –∂–∞–¥–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (4 –±–∞–ª–ª–∞)\n",
        "–í—Å–µ–≥–¥–∞ –ª–∏ –≤—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ ‚Äì¬†—ç—Ç–æ –ª—É—á—à–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞?\n",
        "\n",
        "<details>\n",
        "    <summary>–°–ø–æ–π–ª–µ—Ä</summary>\n",
        "    <p>–ù–µ—Ç</p>\n",
        "</details>\n",
        "\n",
        "**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br><br/> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br><br/> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words <br/> that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br><br/> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps <br/> the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br><br/> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br><br/> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities <br/> by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br><br/> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br><br/> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br><br/> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "–°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ—Ç–∞—Ü–∏—é:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQF4Vc3msKpF"
      },
      "source": [
        "**1. –î–æ–ø–æ–ª–Ω–∏—Ç–µ –º–µ—Ç–æ–¥ `generate` –≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∞—Ç—å —Ç–æ–ø-k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞ –∏ –∏—Ö \"–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\"** (1 –±–∞–ª–ª).   \n",
        "\n",
        "**2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Nucleus Sampling –≤ –º–µ—Ç–æ–¥–µ `generate`** (1 –±–∞–ª–ª)\n",
        "\n",
        "**3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Beam Search** (2 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ü–æ–ª—É—á–∏–ª–æ—Å—å –ª–∏ —É–ª—É—á—à–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfAEfP5kHcc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiksVMOOvO8"
      },
      "source": [
        "## –ü–æ—Å–ª–µ–≤–∫—É—Å–∏–µ (0 –±–∞–ª–ª–æ–≤)\n",
        "\n",
        "–ï—Å–ª–∏ —ç—Ç–∞ –¥–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ –ø–æ–∫–∞–∑–∞–ª–∞—Å—å –≤–∞–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\n",
        "\n",
        "- –æ—Ç –∏–º–µ—é—â–µ–π—Å—è –º–æ–¥–µ–ª–∏ \"–æ—Ç–∫—É—Å–∏—Ç—å\" —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å (–æ—Ç–∫—É—Å–∏—Ç—å —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ—Ç ruT5-small);\n",
        "- –Ω–µ–º–Ω–æ–≥–æ –¥–æ–æ–±—É—á–∏—Ç—å (—á—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è, –ø–æ –≤–∫—É—Å—É);\n",
        "- –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏ \"–≥–ª–∞–∑–∞–º–∏\";\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —Å Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π;\n",
        "- –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å \"–î–∞–µ—Ç –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∏–ª–∏ —ç—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π overkill?\" (–±–∞–∑–æ–≤–æ, –æ—Ç–≤–µ—Ç –ª–µ–∂–∏—Ç –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ üò∏)\n",
        "\n",
        "–ï—â—ë –±–æ–ª–µ–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ:\n",
        "- –ø–æ—á–∏—Ç–∞—Ç—å –ø—Ä–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ (BERT, e.g.)\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ–ª—å–∫–æ Decoder'–æ–º –∏ both Encoder-Decoder'–æ–º;\n",
        "- –≤ —Ç.—á. –ø–æ–¥–æ–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± –æ–Ω–æ –±—ã–ª–æ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π (–∏—Ö, –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤, –±—É–¥–µ—Ç 3 -- —Ç–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä, —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä).\n",
        "\n",
        "*–í–æ–æ–±—â–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å–ª–µ–¥—É–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: \"–¢–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (BERT, e.g.) —Ö–æ—Ä–æ—à–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (–ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤), –ª–∏—à—å –¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (GPT, –Ω–∞–ø—Ä–∏–º–µ—Ä) -- –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (—Å–∫–∞–∂–µ–º, T5) -- –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZM1xLliO1QM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06437165ba564bff9e29aa53f4c0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b11d6c6eae4979a1c8cd42e32730ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3d20b0e457431dbe24120bb4becede",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68b44cb7bddd4a2f950db1a9c00ce066",
            "value": "Map:‚Äá100%"
          }
        },
        "2665d63c206a45d88fada74bc984771e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c29f2dde174a3c8442d9b86a4e3fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665428d410664f94babcd067b879ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b44cb7bddd4a2f950db1a9c00ce066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911fd70fda12476ab2b788433d6ff83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2665d63c206a45d88fada74bc984771e",
            "max": 3048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ddd3783ec047569c2024e461d5ad0f",
            "value": 3048
          }
        },
        "a8e5203a02b845a29f57ca545c50c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09b11d6c6eae4979a1c8cd42e32730ae",
              "IPY_MODEL_911fd70fda12476ab2b788433d6ff83f",
              "IPY_MODEL_bcfc58a3f90745449c3f559aa5ab999a"
            ],
            "layout": "IPY_MODEL_40c29f2dde174a3c8442d9b86a4e3fe9"
          }
        },
        "ab3d20b0e457431dbe24120bb4becede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfc58a3f90745449c3f559aa5ab999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06437165ba564bff9e29aa53f4c0df5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_665428d410664f94babcd067b879ce2e",
            "value": "‚Äá3048/3048‚Äá[00:13&lt;00:00,‚Äá206.31‚Äáexamples/s]"
          }
        },
        "c0ddd3783ec047569c2024e461d5ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
