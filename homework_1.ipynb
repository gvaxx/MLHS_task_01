{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgNFOSlFEzm"
      },
      "source": [
        "# –ú–û–∏–í–° \"–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏\", 5-–π –º–æ–¥—É–ª—å\n",
        "\n",
        "# Homework 1\n",
        "\n",
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å –∫ BERT'—É –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å –∏ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
        "\n",
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ —ç—Ç–æ–º—É –Ω–∞ –æ—Ç–ª–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –º–µ–Ω–µ–µ –∂–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "*–ú—ã —Å—Ä–∞–∑—É –≤–∞—Å –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–≥–∞–µ–º –ø–æ–ø–∞—Å—Ç—å –≤ –ø–µ—Ç–ª—é –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠—Ç–∞ –¥–æ–º–∞—à–∫–∞ –Ω–µ –Ω–∞ –ø—Ä–æ–±–∏—Ç–∏–µ —Å–∫–æ—Ä–∞. –ú—ã –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å, —á—Ç–æ –≤—ã, –≤ —Ü–µ–ª–æ–º, —Å–¥–µ–ª–∞–ª–∏ –≤—Å–µ –≤–µ—Ä–Ω–æ –∏ —Å–º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫—É—é-—Ç–æ –±–æ–ª–µ–µ-–º–µ–Ω–µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω—É—é (—Ç–∞–∫—É—é, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–º–µ—Ç–Ω–æ –ª—É—á—à–µ —Ç–æ–π, —á—Ç–æ –±—ã–ª–∞ –¥–æ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è) –≥–µ–Ω–µ—Ä–∞—Ü–∏—é. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –µ—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ, —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, –Ω–µ –Ω–∞–¥–æ –¥–æ–æ–±—É—á–∞—Ç—å –µ—ë —Å—É—Ç–∫–∞–º–∏. –ù–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞—Å–æ–≤ —Ç–æ—á–Ω–æ –¥–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "–ü–æ –ª—é–±—ã–º –≤–æ–ø—Ä–æ—Å–∞–º –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç—ã –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫–æ —Å–≤–æ–∏–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q-oW4ttVEL_9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (3.0.0)\n",
            "Requirement already satisfied: evaluate in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.4.3)\n",
            "Requirement already satisfied: bert_score in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.3.13)\n",
            "Requirement already satisfied: rouge_score in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from bert_score) (3.8.2)\n",
            "Requirement already satisfied: absl-py in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
            "Requirement already satisfied: nltk in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib->bert_score) (6.1.1)\n",
            "Requirement already satisfied: click in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert_score) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers datasets evaluate bert_score rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ygnbZcjlgJR9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYW38mH0gKX0"
      },
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (0.5 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –¥–∞—Ç–∞—Å–µ—Ç–æ–º —Å ü§ó –ò–ª—å–∏ –ì—É—Å–µ–≤–∞ \"gazeta\". –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–∞—Ä—ã (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ -- –µ–≥–æ —Å–∞–º–º–∞—Ä–∏). –ü–∞—Ä—ã –±—ã–ª–∏ –≤–∑—è—Ç—ã —Å –æ–¥–Ω–æ–∏–º–µ–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –≤ –¥–æ–º–µ–Ω–µ .ru\n",
        "\n",
        "–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å [–∑–¥–µ—Å—å](https://huggingface.co/datasets/IlyaGusev/gazeta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mDV4tJzzB5Hi"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–ø–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('IlyaGusev/gazeta', revision=\"v2.0\", split='train[:5%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjri9a4h6K6"
      },
      "source": [
        "–í—ã –¥–æ–ª–∂–Ω—ã –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç—ã –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å**.\n",
        "\n",
        "–î–æ–±–∞–≤—å—Ç–µ –ø–∞–¥–¥–∏–Ω–≥ –¥–æ `max_length=512` –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–æ `max_length=128` –¥–ª—è –º–µ—Ç–æ–∫.\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—Ä–µ–∑–∫—É —Ç–µ–∫—Å—Ç–æ–≤, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–µ–≤—ã—à–∞–µ—Ç `max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ Bert\n",
        "\n",
        "model_name = 'deepvk/bert-base-uncased' # –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# special_tokens = {'eos_token': '[EOS]'}\n",
        "# tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "def preprocess(examples, use_padding=True):\n",
        "    model_inputs = tokenizer(examples['text'], padding= 'max_length' if use_padding else '', truncation=True, max_length=512)\n",
        "    summary = tokenizer(examples['summary'], padding= 'max_length' if use_padding else '', truncation=True, max_length=128)\n",
        "    model_inputs['labels'] = summary['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a8e5203a02b845a29f57ca545c50c5d0",
            "09b11d6c6eae4979a1c8cd42e32730ae",
            "911fd70fda12476ab2b788433d6ff83f",
            "bcfc58a3f90745449c3f559aa5ab999a",
            "40c29f2dde174a3c8442d9b86a4e3fe9",
            "ab3d20b0e457431dbe24120bb4becede",
            "68b44cb7bddd4a2f950db1a9c00ce066",
            "2665d63c206a45d88fada74bc984771e",
            "c0ddd3783ec047569c2024e461d5ad0f",
            "06437165ba564bff9e29aa53f4c0df5b",
            "665428d410664f94babcd067b879ce2e"
          ]
        },
        "id": "VQxpZ5ivhjlh",
        "outputId": "b3876676-3dc7-4d1d-894e-f0630172afa4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "tokenized_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXQ8gq1UijNj"
      },
      "source": [
        "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–¥–±–∏—Ä–∞—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± —É—Ç–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–æ—Å—Ç—É–ø–Ω–æ–π VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'summary', 'title', 'date', 'url', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3048\n",
              "})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xmMCjFAqSDWR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataloader = DataLoader(splitted_dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(splitted_dataset['test'], batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x16b842c70>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 43, 23, 54 ->\n",
        "# 1,  0,  0,  0 -> 43, 0, 0\n",
        "# 1, 43,  0,  0 -> 43, 23, 0\n",
        "# 1, 43, 23,  0 \n",
        "# 1, 43, 23, 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 0],\n",
              "         [1, 1]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([[[1,0,0], [1,1,0]]])\n",
        "# b = torch.cat([torch.full((a.size()[0], a.size()[1], 1), 100), a[:,:-1]], )\n",
        "\n",
        "# torch.full((3, 1, 1,), 100), a\n",
        "# a\n",
        "# b.T, b.transpose(0,1)\n",
        "a[:,:,:-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0J1iEfFHxRz"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Decoder-c–µ—Ç–∏ (3 –±–∞–ª–ª–∞)\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞**.\n",
        "\n",
        "–ú–æ–∂–µ—Ç–µ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—Ç—å—Å—è –∫–æ–¥–æ–º —Å —Å–µ–º–∏–Ω–∞—Ä–∞ 1 –ø–æ GPT. –í –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ —Å—Ç–æ–∏—Ç (–Ω–æ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) –ø—Ä–æ—è–≤–∏—Ç—å —Å–º–µ–∫–∞–ª–∫—É"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = BertModel.from_pretrained('deepvk/bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.full([a.size()[0], 1], tokenizer.sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "y5qSblF1EMEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import math\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "nn.Transformer\n",
        "# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –¥–µ–∫–æ–¥–µ—Ä–æ–º\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Å—Ä–∞–∑—É –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2, device=device).float() * (-math.log(10000.0) / hidden_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class BertSummarizer(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_decoder_layers=3, num_heads=8, dropout=0.1):\n",
        "        super(BertSummarizer, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å BERT –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tokenizer = tokenizer\n",
        "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥–µ –≤ –¥–µ–∫–æ–¥–µ—Ä\n",
        "        self.embedding = nn.Embedding(self.bert.config.vocab_size, hidden_size).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size)  # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "        # Attention –≥–æ–ª–æ–≤—ã\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout, batch_first=True).to(device),\n",
        "            num_layers=num_decoder_layers,\n",
        "        ).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–µ–∫–æ–¥–µ—Ä –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.fc_out = nn.Linear(hidden_size, self.bert.config.vocab_size).to(device)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        self.softmax = nn.Softmax(dim=2).to(device)\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤–ø–µ—Ä–µ–¥ –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "    def generate_square_subsequent_mask(self, T):\n",
        "        return torch.triu(\n",
        "            torch.full((T, T), float('-inf'), device=device, dtype=torch.float64),  # –ú–∞—Å–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "            diagonal=1,\n",
        "        )\n",
        "\n",
        "    # def shift_decoder_input(self, input_ids):\n",
        "    #     pad_column = torch.full([input_ids.size()[0], 1], self.tokenizer.pad_token_id, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    #     return torch.cat([input_ids[:, :-1], pad_column,], dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
        "        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        decoder_input_ids = decoder_input_ids.to(device)\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state  # –í—ã—Ö–æ–¥—ã BERT –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\n",
        "        \n",
        "        return self.decoder_forward(decoder_input_ids, memory)\n",
        "\n",
        "    def decoder_forward(self, input_ids, memory):\n",
        "        # shifted_ids = self.shift_decoder_input(input_ids)\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "        decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)  # –ú–∞—Å–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "        # print('output_size', output.size())\n",
        "        output = self.fc_out(output)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        # print('output_size', output.size())\n",
        "        return output\n",
        "\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_len=50):\n",
        "        # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        memory = encoder_outputs.last_hidden_state\n",
        "        memory.to(device)\n",
        "        # print('input_ids.size()', input_ids.size())\n",
        "        # print('encoder_outputs.size()', memory.size())\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–æ–∫–µ–Ω–∞ [CLS] –∏–ª–∏ [BOS] (–Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)\n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long).to(device)\n",
        "        \n",
        "        # memory = memory.transpose(0, 1)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedded = self.embedding(decoder_input_ids)\n",
        "            embedded = self.positional_encoding(embedded)\n",
        "\n",
        "            decoder_attention_mask = self.generate_square_subsequent_mask(embedded.size(1)).to(device)\n",
        "            # print('decoder_attention_mask.size()', decoder_attention_mask.size())\n",
        "            \n",
        "            decoder_output = self.decoder(tgt=embedded, memory=memory, tgt_mask=decoder_attention_mask)\n",
        "            # print('decoder_output.size()', decoder_output.size())\n",
        "\n",
        "            output = self.fc_out(decoder_output)\n",
        "            # print('output', output.size())\n",
        "\n",
        "            probs = self.softmax(output)\n",
        "            # print('probs', probs.size())\n",
        "            ids = torch.argmax(probs, dim=2)\n",
        "            # print('ids', ids.size())\n",
        "            # print('decoder_input_ids', decoder_input_ids.size())\n",
        "            decoder_input_ids = torch.cat((decoder_input_ids, ids[:, -1:]), dim=1)\n",
        "\n",
        "\n",
        "            # IndexError: index 0 is out of bounds for dimension 0 with size 0\n",
        "            if decoder_input_ids[0, -1] == tokenizer.sep_token_id:\n",
        "                break\n",
        "        generated_sequence = tokenizer.decode(decoder_input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return generated_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    1,    88, 26278,   520, 31539,  1293, 13095,  2031,   102, 27647,\n",
              "          4074,    16,  5230, 22881,   524, 10178,  1460, 11041,    18,  4676,\n",
              "          1389,  5049, 12059,    88,  9686,  4232,  1207,  3280,  7003,    16,\n",
              "            86,   565,  1088,    16,   617,   848,  9695,  9300,   524,  9535,\n",
              "            16,   827,  3457, 19814,  1049,   796,  2870,  4225,    18,  1114,\n",
              "         27647,   298,    16,  3529, 20885,   282,  3045,   532, 23005,  2418,\n",
              "            16, 14781, 11949,  1013,  8388,   557,    94,  5126,   518,    18,\n",
              "             2,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "             3,     3,     3,     3,     3,     3,     3,     3]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "eval_data_sample['labels'][:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z5VXXCKgecHc"
      },
      "outputs": [],
      "source": [
        "# # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –ø–æ—Å–º–æ—Ä–∏–º –Ω–∞ –µ–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—Ä—É—Ä—É\n",
        "\n",
        "# model = BertSummarizer(bert_model_name=model_name, tokenizer=tokenizer)\n",
        "# model = model.to('cuda')\n",
        "# # model\n",
        "# eval_data_sample = next(iter(eval_dataloader))\n",
        "\n",
        "# model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)\n",
        "# eval_data_sample['input_ids'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H2L-0BmZyu1"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small> 0.25 –±–∞–ª–ª–∞ –∑–∞ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Ä–∞–±–æ—á–∏–π —Ü–∏–∫–ª; </small>\n",
        "\n",
        "<small> +0.5 –±–∞–ª–ª–∞ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ª–æ—Å—Å–∞ –∏ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ç—Ä–µ–π–Ω–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.</small>\n",
        "\n",
        "–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–∏–∫–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(343, 39)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader), len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "us3xiacHBm-U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x14108b910>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # –î–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏–Ω–∞—á–µ CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –µ—ë –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "model = BertSummarizer(bert_model_name=model_name).to(device)\n",
        "\n",
        "\n",
        "def shift_decoder_input(input_ids):\n",
        "    pad_column = torch.full([input_ids.size()[0], 1], tokenizer.pad_token_id, device=device)  # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    return torch.cat([input_ids[:, 1:], pad_column], dim=1)\n",
        "\n",
        "def train_step(model, input_ids, attention_mask, decoder_input_ids, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    \n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    # print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    decoder_input_ids = decoder_input_ids.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    # print('decoder_input_ids.size', decoder_input_ids.size())\n",
        "    optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "    outputs = model(input_ids, attention_mask, decoder_input_ids)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    # logits = outputs.reshape(-1, outputs.size(-1)).to(device)\n",
        "    # labels = labels.reshape(-1).to(device)\n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ—Å—Å, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ output –∏ decoder_input_ids –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ\n",
        "    # print('labels.size', labels.size())\n",
        "\n",
        "    # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "    # labels = labels.reshape(-1)\n",
        "    logits = outputs.reshape(-1, outputs.size(-1))\n",
        "    labels = labels.reshape(-1)\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()  # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def validate_step(model, input_ids, attention_mask, decoder_input_ids, criterion, device):\n",
        "    model.eval()\n",
        "    \n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = decoder_input_ids[:,1:].to(device)\n",
        "    decoder_input_ids = decoder_input_ids[:,:-1]\n",
        "    with torch.no_grad():  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        outputs = model(input_ids, attention_mask, decoder_input_ids)  # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        # outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "        logits = outputs.reshape(-1, outputs.size(-1))\n",
        "        labels = labels.reshape(-1)\n",
        "        loss = criterion(logits, labels)\n",
        "        # loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DataLoader –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
        "train_data_sample = next(iter(train_dataloader))\n",
        "val_data_sample = next(iter(eval_dataloader))  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–∏—Ä—É—é—â–∏–π –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 1\n",
        "plt.ion()  # –í–∫–ª—é—á–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'##ism 189 ##—Ç–µ—Ä–µ –ø—Ä–∏—Å–º–æ—Ç wor –∑–∞–∫–∞—Ç—ã –∫–∞–ª—å—è–Ω –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É –ø–æ–∫—Ä–∞—Å –∫–∞—Ä—Ç–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–∞–≤–ª–æ–≤ 1982 –∫–æ—Å–Ω—É–ª—Å—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –¥–µ–≤—É —Ñ–∏–∫—Å–∞ —Ü–µ–ª—É –¥—Ä—É–∂ –Ω–µ–∂–¥–∞–Ω —Ñ–ª–∞ –æ–∫–Ω–æ —Å—É–¥—è –∫–∞–ª–ª–∏ ##–∂–∏–∫ –∑–∞—â–∏—Ç—É —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è —É–≥–∞–¥—ã –æ—à–∏–±–∞—é—Å—å ##–≥—É—Ä ##–≥—É—Ä ##–≥—É—Ä ##–≥—É—Ä ##–≥—É—Ä ##–≥—É—Ä —Ñ–∏–∫—Å–∞ —Ü–µ–ª—É ##x6bnm –æ—á–µ—Ä http 1940 international ##ple ##–≤–∞–µ–º—ã—Ö –≥–ª–∞–∑ –∫–∞–±–∏ —Ñ–∏–∫—Å–∞ —Ü–µ–ª—É ##246 917'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = model.to('cuda')\n",
        "eval_data_sample = next(iter(eval_dataloader))\n",
        "model.generate(eval_data_sample['input_ids'][:1], eval_data_sample['attention_mask'][:1], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo01OhsoaacU"
      },
      "source": [
        "## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (1 –±–∞–ª–ª)\n",
        "\n",
        "<small>–ü–æ 0.33 –±–∞–ª–ª–∞ –∑–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∫–∞–∂–¥–æ–π –∏–∑ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –º–µ—Ç—Ä–∏–∫</small>\n",
        "\n",
        "**–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫–∏—Ü–∏—é –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.**\n",
        "\n",
        "–î–æ–∫—É–º–µ—Ç–∞—Ü–∏—è –ø–æ –Ω–µ–∫–æ—Ç—Ä—ã–º –º–µ—Ç—Ä–∏–∫–∞–º:\n",
        " 1. [HuggingFace Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        " 2. [HuggingFace Bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        " 3. [HuggingFace BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "BBNcGXt8aSJ2"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [\"hello there\", \"general kenobi\"]\n",
        "bleu = load(\"bleu\")\n",
        "rouge = load('rouge')\n",
        "\n",
        "def compute_metrics(predictions, references):\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
        "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
        "    bertscore_score = bertscore.compute(predictions=predictions, references=references, lang='ru')\n",
        "    return bleu_score, rouge_score, bertscore_score\n",
        "\n",
        "def evaluation(model, tokenizer, dataset):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for i in range(1,len(dataset['input_ids']) + 1):\n",
        "        predictions.append(model.generate(dataset['input_ids'][i-1:i], dataset['attention_mask'][i-1:i], tokenizer))\n",
        "        references.append(dataset['summary'][i-1])\n",
        "    bleu_score, rouge_score, bertscore_score = compute_metrics(predictions, references)\n",
        "    return bleu_score, rouge_score, bertscore_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sprilut/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu': 0.0, 'precisions': [0.0016420361247947454, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.75, 'translation_length': 609, 'reference_length': 348} {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0} {'precision': [0.5261016488075256, 0.480069637298584, 0.5033119916915894, 0.49714815616607666, 0.5058144330978394, 0.5059624910354614, 0.4172086715698242, 0.5419792532920837], 'recall': [0.5760972499847412, 0.5489457249641418, 0.5450485944747925, 0.5260737538337708, 0.5561144351959229, 0.5477374792098999, 0.4364463984966278, 0.606736421585083], 'f1': [0.5499655604362488, 0.5122026205062866, 0.5233494639396667, 0.5112020969390869, 0.5297731757164001, 0.5260218977928162, 0.4266107678413391, 0.5725325345993042], 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.44.2)'}\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "bleu_score, rouge_score, bertscore_score = evaluation(model, tokenizer, eval_data_sample)\n",
        "print(bleu_score, rouge_score, bertscore_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ5GaAZ1chBu"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (0.5 –±–∞–ª–ª–∞)\n",
        "**–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é** (–º–µ—Ç–æ–¥ `.save_pretrained()` –æ–±—ä–µ–∫—Ç–∞ –∫–ª–∞—Å—Å–∞ AutoModel... –∏–ª–∏ `torch.save()`) **–∏ –¥–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**. –£—á—Ç–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –∏–∑–º–µ–Ω—è–ª—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∞ –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), –µ–≥–æ —Ç–æ–∂–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å. –ï—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ\n",
        "\n",
        "–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å ruT5-small –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –¥–∞–µ–º –±–µ–π–∑–ª–∞–π–Ω –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "    running_train_loss = 0.0\n",
        "    running_val_loss = 0.0\n",
        "    \n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ –±–∞—Ç—á–∞–º\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    \n",
        "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
        "    for batch_idx, sample in enumerate(batch_iterator):\n",
        "        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ—Å—Å\n",
        "        loss_item = train_step(\n",
        "            model,\n",
        "            sample['input_ids'], \n",
        "            sample['attention_mask'], \n",
        "            sample['labels'],  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'labels' ‚Äî —ç—Ç–æ —Ü–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "            optimizer, \n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        running_train_loss += loss_item\n",
        "        if (batch_idx % 30 == 0):\n",
        "            train_losses.append(running_train_loss / (batch_idx + 1) )  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ª–æ—Å—Å\n",
        "\n",
        "    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
        "    model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "    total_val_loss = 0.0\n",
        "    for val_batch_idx, val_sample in enumerate(eval_dataloader):\n",
        "        val_loss_item = validate_step(\n",
        "            model,\n",
        "            val_sample['input_ids'],\n",
        "            val_sample['attention_mask'],\n",
        "            val_sample['labels'],  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'labels' ‚Äî —ç—Ç–æ —Ü–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        total_val_loss += val_loss_item\n",
        "    val_loss = total_val_loss / len(eval_dataloader)\n",
        "    val_losses.append(val_loss)\n",
        "    bleu_score, rouge_score, bertscore_score = evaluation(model, tokenizer, eval_data_sample)\n",
        "\n",
        "    clear_output(wait=True)  # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π –≥—Ä–∞—Ñ–∏–∫\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ –¥–ª—è –ª–æ—Å—Å–æ–≤\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss (Epoch {epoch+1})')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # –ì—Ä–∞—Ñ–∏–∫ –¥–ª—è –º–µ—Ç—Ä–∏–∫\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(['BLEU', 'ROUGE-L', 'BERTScore'], [bleu_score['bleu'], rouge_score['rougeL'], np.mean(bertscore_score['f1'])])\n",
        "    plt.title('Evaluation Metrics')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # –°—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å –∑–∞ —ç–ø–æ—Ö—É\n",
        "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
        "    epoch_val_loss = val_loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "    print(f\"BLEU: {bleu_score['bleu']:.4f}, ROUGE-L: {rouge_score['rougeL']:.4f}, BERTScore: {np.mean(bertscore_score['f1']):.4f}\")\n",
        "\n",
        "\n",
        "plt.ioff()  # –û—Ç–∫–ª—é—á–∞–µ–º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KHu9RzbQcceV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "model_save_path = 'my_summarizer_model.pt'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "loaded_model = BertSummarizer(bert_model_name=model_name)\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQH_vj6d2Ue"
      },
      "source": [
        "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–µ –∂–∞–¥–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (4 –±–∞–ª–ª–∞)\n",
        "–í—Å–µ–≥–¥–∞ –ª–∏ –≤—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ ‚Äì¬†—ç—Ç–æ –ª—É—á—à–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞?\n",
        "\n",
        "<details>\n",
        "    <summary>–°–ø–æ–π–ª–µ—Ä</summary>\n",
        "    <p>–ù–µ—Ç</p>\n",
        "</details>\n",
        "\n",
        "**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br><br/> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br><br/> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words <br/> that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br><br/> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps <br/> the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br><br/> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br><br/> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities <br/> by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br><br/> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br><br/> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br><br/> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "–°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ—Ç–∞—Ü–∏—é:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQF4Vc3msKpF"
      },
      "source": [
        "**1. –î–æ–ø–æ–ª–Ω–∏—Ç–µ –º–µ—Ç–æ–¥ `generate` –≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∞—Ç—å —Ç–æ–ø-k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞ –∏ –∏—Ö \"–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\"** (1 –±–∞–ª–ª).   \n",
        "\n",
        "**2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Nucleus Sampling –≤ –º–µ—Ç–æ–¥–µ `generate`** (1 –±–∞–ª–ª)\n",
        "\n",
        "**3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Beam Search** (2 –±–∞–ª–ª–∞)\n",
        "\n",
        "–ü–æ–ª—É—á–∏–ª–æ—Å—å –ª–∏ —É–ª—É—á—à–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JRfAEfP5kHcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BertSummarizerBase(BertSummarizer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(BertSummarizerBase, self).__init__(*args, **kwargs)\n",
        "    def generate(self, input_ids, attention_mask, tokenizer, max_length=100):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        generated = [[] for _ in range(batch_size)]\n",
        "        \n",
        "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.cls_token_id, dtype=torch.long, device=input_ids.device)\n",
        "        for _ in range(max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
        "            next_token_logits = outputs[:, -1, :]\n",
        "            \n",
        "            next_tokens = self.get_next_token(next_token_logits)\n",
        "            \n",
        "            for i, token in enumerate(next_tokens.squeeze(1)):\n",
        "                generated[i].append(token.item())\n",
        "            \n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, next_tokens], dim=-1)\n",
        "            \n",
        "            if all(tokenizer.eos_token_id in seq for seq in generated):\n",
        "                break\n",
        "        \n",
        "        return [tokenizer.decode(seq, skip_special_tokens=True) for seq in generated]\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        raise NotImplementedError(\"This method should be implemented in derived classes.\")\n",
        "\n",
        "class BertSummarizerGreedy(BertSummarizerBase):\n",
        "    def get_next_token(self, logits):\n",
        "        return torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "\n",
        "class BertSummarizerTopK(BertSummarizerBase):\n",
        "    def __init__(self, *args, k=5, **kwargs):\n",
        "        super(BertSummarizerTopK, self).__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, self.k, dim=-1)\n",
        "        top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
        "        return top_k_indices[:, 0].unsqueeze(1)\n",
        "\n",
        "class BertSummarizerNucleusSampling(BertSummarizerBase):\n",
        "    def __init__(self, *args, p=0.9, **kwargs):\n",
        "        super(BertSummarizerNucleusSampling, self).__init__(*args, **kwargs)\n",
        "        self.p = p\n",
        "\n",
        "    def get_next_token(self, logits):\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > self.p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        filtered_logits = torch.where(indices_to_remove, torch.ones_like(logits) * float('-inf'), logits)\n",
        "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
        "        return torch.multinomial(probabilities, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries:   0%|          | 0/39 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries:   3%|‚ñé         | 1/39 [01:14<47:29, 74.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n",
            "input_ids.size torch.Size([8, 512])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries:   3%|‚ñé         | 1/39 [01:57<1:14:12, 117.16s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model_base \u001b[38;5;241m=\u001b[39m BertSummarizerGreedy(bert_model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m     24\u001b[0m model_base\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_save_path))\n\u001b[0;32m---> 25\u001b[0m summaries_base \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# references = [example['summary'] for example in eval_dataloader[:len(summaries_base)]]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m scores_base \u001b[38;5;241m=\u001b[39m compute_metrics(summaries_base, references)\n",
            "Cell \u001b[0;32mIn[61], line 14\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(generated)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n",
            "Cell \u001b[0;32mIn[60], line 14\u001b[0m, in \u001b[0;36mBertSummarizerBase.generate\u001b[0;34m(self, input_ids, attention_mask, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     17\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_token(next_token_logits)\n",
            "Cell \u001b[0;32mIn[52], line 58\u001b[0m, in \u001b[0;36mBertSummarizer.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids)\u001b[0m\n\u001b[1;32m     56\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m decoder_input_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids.size\u001b[39m\u001b[38;5;124m'\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 58\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m memory \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# –í—ã—Ö–æ–¥—ã BERT –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥–µ–∫–æ–¥–µ—Ä–µ\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_forward(decoder_input_ids, memory)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏\n",
        "def generate_summaries(model, dataloader):\n",
        "    summaries = []\n",
        "    for batch in tqdm(dataloader, desc=\"Generating summaries\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        generated = model.generate(input_ids, attention_mask, tokenizer, max_length=50)\n",
        "        summaries.extend(generated)\n",
        "    return summaries\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "model_base = BertSummarizerGreedy(bert_model_name=model_name)\n",
        "model_base.load_state_dict(torch.load(model_save_path))\n",
        "summaries_base = generate_summaries(model_base, eval_dataloader)\n",
        "# references = [example['summary'] for example in eval_dataloader[:len(summaries_base)]]\n",
        "scores_base = compute_metrics(summaries_base, references)\n",
        "\n",
        "# print(\"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:\")\n",
        "# print(scores_base)\n",
        "\n",
        "# # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å Top-K —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
        "# model_top_k = BertSummarizerTopK(bert_model_name=model_name)\n",
        "# model_top_k.load_state_dict(torch.load(model_save_path))\n",
        "# summaries_top_k = generate_summaries(model_top_k, eval_dataloader)\n",
        "# scores_top_k = compute_metrics(summaries_top_k, references)\n",
        "\n",
        "# print(\"\\n–ú–æ–¥–µ–ª—å —Å Top-K —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º:\")\n",
        "# print(scores_top_k)\n",
        "\n",
        "# # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å Nucleus —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
        "# model_nucleus = BertSummarizerNucleusSampling(bert_model_name=model_name)\n",
        "# model_nucleus.load_state_dict(torch.load(model_save_path))\n",
        "# summaries_nucleus = generate_summaries(model_nucleus, eval_dataloader)\n",
        "# scores_nucleus = compute_metrics(summaries_nucleus, references)\n",
        "\n",
        "# print(\"\\n–ú–æ–¥–µ–ª—å —Å Nucleus —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º:\")\n",
        "# print(scores_nucleus)\n",
        "\n",
        "# # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "# print(\"\\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
        "# for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "#     print(f\"{metric}:\")\n",
        "#     print(f\"  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {scores_base[metric]:.4f}\")\n",
        "#     print(f\"  Top-K: {scores_top_k[metric]:.4f}\")\n",
        "#     print(f\"  Nucleus: {scores_nucleus[metric]:.4f}\")\n",
        "\n",
        "# # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã\n",
        "# metrics = ['rouge1', 'rouge2', 'rougeL']\n",
        "# models = ['–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å', 'Top-K', 'Nucleus']\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.25\n",
        "\n",
        "# ax.bar(x - width, [scores_base[m] for m in metrics], width, label='–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å')\n",
        "# ax.bar(x, [scores_top_k[m] for m in metrics], width, label='Top-K')\n",
        "# ax.bar(x + width, [scores_nucleus[m] for m in metrics], width, label='Nucleus')\n",
        "\n",
        "# ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')\n",
        "# ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ROUGE –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π')\n",
        "# ax.set_xticks(x)\n",
        "# ax.set_xticklabels(metrics)\n",
        "# ax.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiksVMOOvO8"
      },
      "source": [
        "## –ü–æ—Å–ª–µ–≤–∫—É—Å–∏–µ (0 –±–∞–ª–ª–æ–≤)\n",
        "\n",
        "–ï—Å–ª–∏ —ç—Ç–∞ –¥–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ –ø–æ–∫–∞–∑–∞–ª–∞—Å—å –≤–∞–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\n",
        "\n",
        "- –æ—Ç –∏–º–µ—é—â–µ–π—Å—è –º–æ–¥–µ–ª–∏ \"–æ—Ç–∫—É—Å–∏—Ç—å\" —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä–Ω—É—é —á–∞—Å—Ç—å (–æ—Ç–∫—É—Å–∏—Ç—å —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ—Ç ruT5-small);\n",
        "- –Ω–µ–º–Ω–æ–≥–æ –¥–æ–æ–±—É—á–∏—Ç—å (—á—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è, –ø–æ –≤–∫—É—Å—É);\n",
        "- –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏ \"–≥–ª–∞–∑–∞–º–∏\";\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —Å Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π;\n",
        "- –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å \"–î–∞–µ—Ç –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∏–ª–∏ —ç—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π overkill?\" (–±–∞–∑–æ–≤–æ, –æ—Ç–≤–µ—Ç –ª–µ–∂–∏—Ç –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ üò∏)\n",
        "\n",
        "–ï—â—ë –±–æ–ª–µ–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ:\n",
        "- –ø–æ—á–∏—Ç–∞—Ç—å –ø—Ä–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ (BERT, e.g.)\n",
        "- —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ–ª—å–∫–æ Decoder'–æ–º –∏ both Encoder-Decoder'–æ–º;\n",
        "- –≤ —Ç.—á. –ø–æ–¥–æ–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–± –æ–Ω–æ –±—ã–ª–æ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π (–∏—Ö, –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤, –±—É–¥–µ—Ç 3 -- —Ç–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä, —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä).\n",
        "\n",
        "*–í–æ–æ–±—â–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å–ª–µ–¥—É–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: \"–¢–æ–ª—å–∫–æ —ç–Ω–∫–æ–¥–µ—Ä–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (BERT, e.g.) —Ö–æ—Ä–æ—à–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (–ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤), –ª–∏—à—å –¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (GPT, –Ω–∞–ø—Ä–∏–º–µ—Ä) -- –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–Ω—ã–µ (—Å–∫–∞–∂–µ–º, T5) -- –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZM1xLliO1QM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06437165ba564bff9e29aa53f4c0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b11d6c6eae4979a1c8cd42e32730ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3d20b0e457431dbe24120bb4becede",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68b44cb7bddd4a2f950db1a9c00ce066",
            "value": "Map:‚Äá100%"
          }
        },
        "2665d63c206a45d88fada74bc984771e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c29f2dde174a3c8442d9b86a4e3fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665428d410664f94babcd067b879ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b44cb7bddd4a2f950db1a9c00ce066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911fd70fda12476ab2b788433d6ff83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2665d63c206a45d88fada74bc984771e",
            "max": 3048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ddd3783ec047569c2024e461d5ad0f",
            "value": 3048
          }
        },
        "a8e5203a02b845a29f57ca545c50c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09b11d6c6eae4979a1c8cd42e32730ae",
              "IPY_MODEL_911fd70fda12476ab2b788433d6ff83f",
              "IPY_MODEL_bcfc58a3f90745449c3f559aa5ab999a"
            ],
            "layout": "IPY_MODEL_40c29f2dde174a3c8442d9b86a4e3fe9"
          }
        },
        "ab3d20b0e457431dbe24120bb4becede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfc58a3f90745449c3f559aa5ab999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06437165ba564bff9e29aa53f4c0df5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_665428d410664f94babcd067b879ce2e",
            "value": "‚Äá3048/3048‚Äá[00:13&lt;00:00,‚Äá206.31‚Äáexamples/s]"
          }
        },
        "c0ddd3783ec047569c2024e461d5ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
